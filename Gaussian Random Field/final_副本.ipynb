{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36203404",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1757541506892,
     "user": {
      "displayName": "Shanzi Bao",
      "userId": "16567399982080387951"
     },
     "user_tz": -480
    },
    "id": "36203404",
    "outputId": "e63d1382-a8da-429a-dae2-728f273795e8"
   },
   "outputs": [],
   "source": [
    "# All your existing imports\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from config import create_range_parameter_config\n",
    "from dataset import your_prior_sampler, your_simulator\n",
    "from nn_model import NCMLP\n",
    "from sde import get_sde\n",
    "from train import train_score_network\n",
    "from cnf import CNF\n",
    "\n",
    "# MAIN CONFIGURATION - Single place to control everything\n",
    "CONFIG = {\n",
    "    'EXPERIMENT_MODE': 'quick',  # 'quick' or 'full'\n",
    "    'RANDOM_SEED': 42,\n",
    "\n",
    "    # Training data\n",
    "    'n_training_samples': 100000,  # 1000 for quick, 5000 for full\n",
    "\n",
    "    # Test parameters\n",
    "    'test_theta_values': [15, 45, 75],  # Will expand for full mode\n",
    "    'n_experiments_per_theta': 5,       # 5 for quick, 20 for full\n",
    "    'n_posterior_samples': 10000,\n",
    "\n",
    "    # Methods to compare\n",
    "    'methods_to_test': ['raw', 'data_score', 'pairwise_grouped', 'combined_grouped', 'abc', 'abc_adjusted'],\n",
    "\n",
    "    # Output settings\n",
    "    'save_results': True,\n",
    "    'plot_results': True,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "# Auto-expand for full experiment\n",
    "if CONFIG['EXPERIMENT_MODE'] == 'full':\n",
    "    CONFIG.update({\n",
    "        'n_training_samples': 100000,\n",
    "        'test_theta_values': list(range(5, 100, 10)),\n",
    "        'n_experiments_per_theta': 20\n",
    "    })\n",
    "\n",
    "print(f\"Running {CONFIG['EXPERIMENT_MODE']} experiment\")\n",
    "print(f\"Testing methods: {CONFIG['methods_to_test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f376e8",
   "metadata": {
    "id": "b6f376e8"
   },
   "outputs": [],
   "source": [
    "def estimate_range_by_correlation(x_obs):\n",
    "    \"\"\"Estimate range parameter based on spatial correlation of neighboring points in 8x8 grid\"\"\"\n",
    "    locations = create_2d_grid()  # (64, 2) - Grid coordinates\n",
    "\n",
    "    # Compute empirical correlation proxy for neighboring points\n",
    "    x_normalized = (x_obs - jnp.mean(x_obs)) / (jnp.std(x_obs) + 1e-8)\n",
    "\n",
    "    # Use multiple neighbor pairs for correlation estimation - adapted for 8x8 grid\n",
    "    neighbor_pairs = [\n",
    "        # Horizontal neighbors (within same row)\n",
    "        (0,1), (1,2), (2,3), (3,4), (4,5), (5,6), (6,7),  # Row 1\n",
    "        (8,9), (9,10), (10,11), (11,12), (12,13), (13,14), (14,15),  # Row 2\n",
    "        (16,17), (24,25), (32,33), (40,41), (48,49), (56,57),  # Sampled from other rows\n",
    "        # Vertical neighbors (within same column)\n",
    "        (0,8), (1,9), (2,10), (3,11), (4,12), (5,13), (6,14), (7,15),  # Rows 1-2\n",
    "        (8,16), (9,17), (16,24), (17,25), (24,32), (32,40), (40,48), (48,56),  # Sampled from other columns\n",
    "        # Diagonal neighbors\n",
    "        (0,9), (1,10), (2,11), (8,17), (9,18), (16,25), (24,33)\n",
    "    ]\n",
    "\n",
    "    correlations = []\n",
    "    distances = []\n",
    "\n",
    "    for i, j in neighbor_pairs:\n",
    "        if i < 64 and j < 64:  # Ensure indices are within 64-point range\n",
    "            corr = x_normalized[i] * x_normalized[j]\n",
    "            dist = jnp.sqrt(jnp.sum((locations[i] - locations[j])**2))  # 2D Euclidean distance\n",
    "            if corr > 0.01:\n",
    "                correlations.append(corr)\n",
    "                distances.append(dist)\n",
    "\n",
    "    if correlations:\n",
    "        avg_corr = jnp.mean(jnp.array(correlations))\n",
    "        avg_dist = jnp.mean(jnp.array(distances))\n",
    "\n",
    "        # Reverse-estimate range: corr = exp(-dist/range)\n",
    "        range_est = -avg_dist / jnp.log(avg_corr)\n",
    "\n",
    "        # Constrain to reasonable range\n",
    "        return float(jnp.clip(range_est, 5.0, 95.0))\n",
    "\n",
    "    # If correlations are too small, use prior mean\n",
    "    return 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591f439",
   "metadata": {
    "id": "7591f439"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "from collections import defaultdict\n",
    "\n",
    "@jit(nopython=True)\n",
    "def create_2d_grid_numba():\n",
    "    \"\"\"Create 8x8 grid points - numba optimized version\"\"\"\n",
    "    coords = np.linspace(0, 1, 8)\n",
    "    locations = np.zeros((64, 2))\n",
    "    idx = 0\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            locations[idx, 0] = coords[i]\n",
    "            locations[idx, 1] = coords[j]\n",
    "            idx += 1\n",
    "    return locations\n",
    "\n",
    "@jit(nopython=True)\n",
    "def estimate_range_by_correlation_numba(x_obs):\n",
    "    \"\"\"Estimate range parameter based on 8x8 neighboring points correlation - numba optimized version\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # Calculate empirical correlation proxy for neighboring points\n",
    "    x_mean = np.mean(x_obs)\n",
    "    x_std = np.std(x_obs)\n",
    "    if x_std < 1e-8:\n",
    "        x_std = 1e-8\n",
    "    x_normalized = (x_obs - x_mean) / x_std\n",
    "\n",
    "    # Define neighboring point pairs - use static array instead of list comprehension\n",
    "    neighbor_pairs = np.array([\n",
    "        # Horizontal neighbors\n",
    "        [0,1], [1,2], [2,3], [3,4], [4,5], [5,6], [6,7],\n",
    "        [8,9], [9,10], [10,11], [11,12], [12,13], [13,14], [14,15],\n",
    "        [16,17], [17,18], [18,19], [19,20], [20,21], [21,22], [22,23],\n",
    "        [24,25], [25,26], [26,27], [27,28], [28,29], [29,30], [30,31],\n",
    "        [32,33], [33,34], [34,35], [35,36], [36,37], [37,38], [38,39],\n",
    "        [40,41], [41,42], [42,43], [43,44], [44,45], [45,46], [46,47],\n",
    "        [48,49], [49,50], [50,51], [51,52], [52,53], [53,54], [54,55],\n",
    "        [56,57], [57,58], [58,59], [59,60], [60,61], [61,62], [62,63],\n",
    "        # Vertical neighbors\n",
    "        [0,8], [1,9], [2,10], [3,11], [4,12], [5,13], [6,14], [7,15],\n",
    "        [8,16], [9,17], [10,18], [11,19], [12,20], [13,21], [14,22], [15,23],\n",
    "        [16,24], [17,25], [18,26], [19,27], [20,28], [21,29], [22,30], [23,31],\n",
    "        [24,32], [25,33], [26,34], [27,35], [28,36], [29,37], [30,38], [31,39],\n",
    "        [32,40], [33,41], [34,42], [35,43], [36,44], [37,45], [38,46], [39,47],\n",
    "        [40,48], [41,49], [42,50], [43,51], [44,52], [45,53], [46,54], [47,55],\n",
    "        [48,56], [49,57], [50,58], [51,59], [52,60], [53,61], [54,62], [55,63],\n",
    "        # Diagonal neighbors (partial)\n",
    "        [0,9], [1,10], [2,11], [3,12], [4,13], [5,14], [6,15],\n",
    "        [8,17], [9,18], [10,19], [11,20], [12,21], [13,22], [14,23],\n",
    "        [16,25], [17,26], [18,27], [19,28], [20,29], [21,30], [22,31]\n",
    "    ])\n",
    "\n",
    "    correlations = []\n",
    "    distances = []\n",
    "\n",
    "    for k in range(neighbor_pairs.shape[0]):\n",
    "        i, j = neighbor_pairs[k, 0], neighbor_pairs[k, 1]\n",
    "        if i < 64 and j < 64:\n",
    "            corr = x_normalized[i] * x_normalized[j]\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            dist = np.sqrt(dist_sq)\n",
    "            if corr > 0.01:\n",
    "                correlations.append(corr)\n",
    "                distances.append(dist)\n",
    "\n",
    "    if len(correlations) > 0:\n",
    "        avg_corr = np.mean(np.array(correlations))\n",
    "        avg_dist = np.mean(np.array(distances))\n",
    "\n",
    "        # Reverse-estimate range: corr = exp(-dist/range)\n",
    "        if avg_corr > 0:\n",
    "            range_est = -avg_dist / np.log(avg_corr)\n",
    "            # Constrain to reasonable range\n",
    "            return min(max(range_est, 5.0), 95.0)\n",
    "\n",
    "    # If correlations are too small, use prior mean\n",
    "    return 50.0\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_all_distances_and_groups_numba():\n",
    "    \"\"\"Calculate all distances and create groups - numba optimized version\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # Pre-compute all distances\n",
    "    distances = np.zeros((64, 64))\n",
    "    for i in range(64):\n",
    "        for j in range(64):\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            distances[i, j] = np.sqrt(dist_sq)\n",
    "\n",
    "    # Find unique distances (using simple deduplication logic)\n",
    "    unique_distances = []\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    for i in range(64):\n",
    "        for j in range(i+1, 64):\n",
    "            dist = distances[i, j]\n",
    "            is_unique = True\n",
    "            for existing_dist in unique_distances:\n",
    "                if abs(dist - existing_dist) < tolerance:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                unique_distances.append(dist)\n",
    "\n",
    "    # Sort distances\n",
    "    unique_distances.sort()\n",
    "\n",
    "    return distances, np.array(unique_distances)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_grouped_scores_for_sample_numba(x, distances, unique_distances, range_param, sigma=5.0):\n",
    "    \"\"\"Calculate grouped scores for single sample - numba optimized version\"\"\"\n",
    "    group_scores = np.zeros(len(unique_distances))\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    for dist_idx in prange(len(unique_distances)):\n",
    "        target_dist = unique_distances[dist_idx]\n",
    "        group_score_sum = 0.0\n",
    "\n",
    "        # Find all point pairs matching this distance\n",
    "        for i in range(64):\n",
    "            for j in range(i+1, 64):\n",
    "                actual_dist = distances[i, j]\n",
    "                if abs(actual_dist - target_dist) < tolerance:\n",
    "                    # Calculate pairwise difference\n",
    "                    v_ij = x[i] - x[j]\n",
    "\n",
    "                    # Calculate semivariogram Î³(d; Î¸) = ÏƒÂ²(1 - exp(-d/Î¸))\n",
    "                    exp_term = np.exp(-target_dist / range_param)\n",
    "                    gamma_ij = sigma**2 * (1 - exp_term)\n",
    "                    gamma_ij = max(gamma_ij, 1e-8)  # Avoid numerical issues\n",
    "\n",
    "                    # Calculate derivative of Î³ with respect to range_param\n",
    "                    dgamma_dtheta = sigma**2 * exp_term * target_dist / (range_param**2)\n",
    "\n",
    "                    # Calculate pairwise score (based on paper formula 5.8)\n",
    "                    score_ij = dgamma_dtheta / (2 * gamma_ij) * (v_ij**2 / (2 * gamma_ij) - 1)\n",
    "\n",
    "                    group_score_sum += score_ij\n",
    "\n",
    "        group_scores[dist_idx] = group_score_sum\n",
    "\n",
    "    return group_scores\n",
    "\n",
    "def compute_distance_grouped_pairwise_scores_estimated_theta_numba(x_samples, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate distance-grouped pairwise scores using estimated theta (based on pairwise differences) - numba optimized 64-point version\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Calculating distance-grouped pairwise scores using estimated theta (based on pairwise differences, 64 points, numba optimized)...\")\n",
    "\n",
    "    # Pre-compute distance information\n",
    "    distances, unique_distances = compute_all_distances_and_groups_numba()\n",
    "\n",
    "    # Calculate grouped scores for each sample\n",
    "    all_grouped_scores = []\n",
    "    sigma = 5.0  # Standard deviation of spatial process\n",
    "\n",
    "    for sample_idx in range(len(x_samples)):\n",
    "        x = np.array(x_samples[sample_idx])\n",
    "\n",
    "        # Estimate theta instead of using true theta\n",
    "        range_param = estimate_range_by_correlation_numba(x)\n",
    "\n",
    "        # Calculate grouped scores for this sample\n",
    "        group_scores = compute_grouped_scores_for_sample_numba(x, distances, unique_distances, range_param, sigma)\n",
    "\n",
    "        all_grouped_scores.append(group_scores)\n",
    "\n",
    "    result = np.array(all_grouped_scores)\n",
    "    print(f\"âœ… 64-point version grouped pairwise scores calculation completed, output dimensions: {result.shape}\")\n",
    "    print(f\"   {len(unique_distances)} dimensions correspond to aggregated scores of point pairs at different 8x8 distances\")\n",
    "    print(f\"   Distance groups include: {[round(d, 3) for d in unique_distances]}\")\n",
    "\n",
    "    return result, unique_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5ffbf",
   "metadata": {
    "id": "f4d5ffbf"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Optimized version - compute pairwise first, then sum to get data score\n",
    "def prepare_all_data_correlated_simplified_optimized(n_samples, key=42):\n",
    "    \"\"\"Prepare correlated data (optimized: compute pairwise first, then sum) - 64-point version\"\"\"\n",
    "    print(\"Preparing optimized correlated data comparison (64 points)...\")\n",
    "    key = jr.PRNGKey(key)\n",
    "\n",
    "    # Generate correlated raw data\n",
    "    theta_train = your_prior_sampler(n_samples, key)\n",
    "    key, *sim_keys = jr.split(key, n_samples + 1)\n",
    "    x_train = jnp.array([your_simulator(theta_train[i], sim_keys[i])\n",
    "                         for i in range(n_samples)])\n",
    "\n",
    "    # Check data correlation\n",
    "    print(\"\\nChecking data correlation...\")\n",
    "    corr_matrix = jnp.corrcoef(x_train.T)\n",
    "    off_diagonal = corr_matrix - jnp.diag(jnp.diag(corr_matrix))\n",
    "    max_correlation = jnp.max(jnp.abs(off_diagonal))\n",
    "    print(f\"Maximum off-diagonal correlation coefficient: {max_correlation:.4f}\")\n",
    "    if max_correlation > 0.3:\n",
    "        print(\"Data confirmed to be correlated\")\n",
    "    else:\n",
    "        print(\"Warning: Data correlation is weak\")\n",
    "\n",
    "    print(\"\\nComputing training features (optimized: compute pairwise first, then sum)...\")\n",
    "\n",
    "    # Core optimization: compute pairwise scores grouped by distance first\n",
    "    print(\"   Step 1: Compute grouped pairwise features...\")\n",
    "    x_train_pairwise_grouped, distance_groups = compute_distance_grouped_pairwise_scores_estimated_theta_numba(x_train)\n",
    "\n",
    "    # Then simply sum to get data score (avoid recomputing 2016 pairs!)\n",
    "    print(\"   Step 2: Sum pairwise features to get data score...\")\n",
    "    x_train_data_scores = jnp.sum(x_train_pairwise_grouped, axis=1, keepdims=True)  # (n_samples, 1)\n",
    "\n",
    "    print(f\"Computation completed:\")\n",
    "    print(f\"   - Pairwise scores for 2016 pairs computed only once\")\n",
    "    print(f\"   - Data Score = sum across distance groups\")\n",
    "    print(f\"   - Computation time significantly reduced\")\n",
    "\n",
    "    # Step 3. Create combined features - simply concatenate\n",
    "    print(\"   Step 3: Create combined features...\")\n",
    "\n",
    "    # Normalize pairwise features\n",
    "    normalized_pairwise = []\n",
    "    for i in range(len(x_train_pairwise_grouped)):\n",
    "        sample_vec = x_train_pairwise_grouped[i]  # vector of distance groups\n",
    "        normalized_vec = (sample_vec - jnp.mean(sample_vec)) / (jnp.std(sample_vec) + 1e-8)\n",
    "        normalized_pairwise.append(normalized_vec)\n",
    "    x_train_pairwise_grouped = jnp.array(normalized_pairwise)\n",
    "\n",
    "    # Concatenate data_score + normalized_pairwise\n",
    "    x_train_combined_grouped = jnp.concatenate([x_train_data_scores, x_train_pairwise_grouped], axis=1)\n",
    "\n",
    "    # Normalize combined features\n",
    "    normalized_combined = []\n",
    "    for i in range(len(x_train_combined_grouped)):\n",
    "        sample_vec = x_train_combined_grouped[i]  # (1 + number of distance groups)-dim vector\n",
    "        normalized_vec = (sample_vec - jnp.mean(sample_vec)) / (jnp.std(sample_vec) + 1e-8)\n",
    "        normalized_combined.append(normalized_vec)\n",
    "    x_train_combined_grouped = jnp.array(normalized_combined)\n",
    "\n",
    "    print(f\"\\n64-point optimized training data preparation completed:\")\n",
    "    print(f\"   - Parameter shape: {theta_train.shape}\")\n",
    "    print(f\"   - Raw data shape: {x_train.shape}\")\n",
    "    print(f\"   - Data Score shape: {x_train_data_scores.shape} (obtained via pairwise summation)\")\n",
    "    print(f\"   - Pairwise Grouped shape: {x_train_pairwise_grouped.shape}\")\n",
    "    print(f\"   - Combined Grouped shape: {x_train_combined_grouped.shape}\")\n",
    "    print(f\"   - Number of distance groups: {len(distance_groups)}\")\n",
    "\n",
    "    print(f\"\\nOptimization effect:\")\n",
    "    print(f\"   - Original version: required 2016*2 = 4032 pairwise score computations\")\n",
    "    print(f\"   - Optimized version: only 2016*1 = 2016 computations\")\n",
    "    print(f\"   - Speed improvement: ~2x\")\n",
    "\n",
    "    return (theta_train, x_train, x_train_data_scores,\n",
    "            x_train_pairwise_grouped, x_train_combined_grouped, distance_groups)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaeda24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272176,
     "status": "ok",
     "timestamp": 1757542022057,
     "user": {
      "displayName": "Shanzi Bao",
      "userId": "16567399982080387951"
     },
     "user_tz": -480
    },
    "id": "daaeda24",
    "outputId": "60280fec-d779-4262-b326-a5fa019a6dd3"
   },
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    \"\"\"Generate training data using existing functions\"\"\"\n",
    "    print(f\"Generating {CONFIG['n_training_samples']} training samples...\")\n",
    "\n",
    "    key = jr.PRNGKey(CONFIG['RANDOM_SEED'])\n",
    "    theta_train = your_prior_sampler(CONFIG['n_training_samples'], key)\n",
    "\n",
    "    key, *sim_keys = jr.split(key, CONFIG['n_training_samples'] + 1)\n",
    "    x_train = jnp.array([your_simulator(theta_train[i], sim_keys[i])\n",
    "                        for i in range(CONFIG['n_training_samples'])])\n",
    "\n",
    "    print(f\"Training data shape: theta={theta_train.shape}, x={x_train.shape}\")\n",
    "    return theta_train, x_train\n",
    "\n",
    "# Generate once, use for all methods\n",
    "theta_train, x_train = generate_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbec04f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384027,
     "status": "ok",
     "timestamp": 1757542406098,
     "user": {
      "displayName": "Shanzi Bao",
      "userId": "16567399982080387951"
     },
     "user_tz": -480
    },
    "id": "cdbec04f",
    "outputId": "2a8bd316-ef52-4b10-d1e1-66d864a3cd43"
   },
   "outputs": [],
   "source": [
    "def prepare_all_features(theta_train, x_train):\n",
    "    \"\"\"Prepare all feature types using existing optimized functions\"\"\"\n",
    "    print(\"Preparing features for all methods...\")\n",
    "\n",
    "    # Use your existing optimized feature computation\n",
    "    (theta_train, x_train, x_train_data_scores,\n",
    "     x_train_pairwise_grouped, x_train_combined_grouped,\n",
    "     distance_groups) = prepare_all_data_correlated_simplified_optimized(\n",
    "        n_samples=CONFIG['n_training_samples'],\n",
    "        key=CONFIG['RANDOM_SEED']\n",
    "    )\n",
    "\n",
    "    features = {\n",
    "        'raw': x_train,\n",
    "        'data_score': x_train_data_scores,\n",
    "        'pairwise_grouped': x_train_pairwise_grouped,\n",
    "        'combined_grouped': x_train_combined_grouped\n",
    "    }\n",
    "\n",
    "    print(\"Feature dimensions:\")\n",
    "    for name, feat in features.items():\n",
    "        print(f\"  {name}: {feat.shape}\")\n",
    "\n",
    "    return features\n",
    "\n",
    "features = prepare_all_features(theta_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c15973",
   "metadata": {
    "id": "c8c15973"
   },
   "outputs": [],
   "source": [
    "def train_single_method(method_name, theta_train, x_input, train_key, dim_data=64):  # ğŸ”¥ changed from 16 to 64\n",
    "    \"\"\"Train a single method\"\"\"\n",
    "    print(f\"\\n Training {method_name} method...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    key = jr.PRNGKey(train_key)\n",
    "    config = create_range_parameter_config()\n",
    "\n",
    "    # Adjust data dimension\n",
    "    config.algorithm.dim_data = dim_data\n",
    "\n",
    "    sde = get_sde(config)\n",
    "    model = NCMLP(key, config)\n",
    "\n",
    "    trained_model, ds_mean, ds_std = train_score_network(\n",
    "        config, model, sde, theta_train, x_input, key\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"{method_name} training completed, time taken: {training_time:.2f} seconds\")\n",
    "\n",
    "    return {\n",
    "        'trained_model': trained_model,\n",
    "        'ds_mean': ds_mean,\n",
    "        'ds_std': ds_std,\n",
    "        'sde': sde,\n",
    "        'config': config,\n",
    "        'training_time': training_time,\n",
    "        'method': method_name\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1dea83",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a1dea83"
   },
   "outputs": [],
   "source": [
    "def train_all_cnf_methods(theta_train, features):\n",
    "    \"\"\"Train CNF models for all feature types\"\"\"\n",
    "    print(\"Training CNF models...\")\n",
    "\n",
    "    trained_models = {}\n",
    "\n",
    "    for method_name in ['raw', 'data_score', 'pairwise_grouped', 'combined_grouped']:\n",
    "        if method_name not in CONFIG['methods_to_test']:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTraining {method_name} model...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use your existing training function\n",
    "        x_input = features[method_name]\n",
    "        dim_data = x_input.shape[1]\n",
    "\n",
    "        trained_models[method_name] = train_single_method(\n",
    "            method_name, theta_train, x_input,\n",
    "            train_key=CONFIG['RANDOM_SEED'],\n",
    "            dim_data=dim_data\n",
    "        )\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.1f}s\")\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "trained_models = train_all_cnf_methods(theta_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfed5e1",
   "metadata": {
    "id": "3bfed5e1",
    "outputId": "e1549851-b7f7-44ef-b3ec-6f90b50e3a5f"
   },
   "outputs": [],
   "source": [
    "def create_cnf_samplers(trained_models):\n",
    "    \"\"\"Create CNF samplers from trained models\"\"\"\n",
    "    print(\"Creating CNF samplers...\")\n",
    "\n",
    "    cnf_samplers = {}\n",
    "\n",
    "    for method_name, model_info in trained_models.items():\n",
    "        cnf = CNF(\n",
    "            score_network=model_info['trained_model'],\n",
    "            sde=model_info['sde'],\n",
    "            ds_means=model_info['ds_mean'],\n",
    "            ds_stds=model_info['ds_std']\n",
    "        )\n",
    "\n",
    "        cnf_samplers[method_name] = {\n",
    "            'cnf': cnf,\n",
    "            'config': model_info['config'],\n",
    "            'training_time': model_info['training_time']\n",
    "        }\n",
    "\n",
    "    print(f\"Created {len(cnf_samplers)} CNF samplers\")\n",
    "    return cnf_samplers\n",
    "\n",
    "cnf_samplers = create_cnf_samplers(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67166b7b",
   "metadata": {
    "id": "67166b7b"
   },
   "outputs": [],
   "source": [
    "def run_single_inference(method_name, x_obs, cnf_samplers=None):\n",
    "    \"\"\"Run inference for a single method\"\"\"\n",
    "\n",
    "    if method_name in ['abc', 'abc_adjusted']:\n",
    "        # Use your existing ABC functions\n",
    "        use_adjustment = (method_name == 'abc_adjusted')\n",
    "        samples = run_abc_inference_with_adjustment(\n",
    "            x_obs,\n",
    "            n_samples=CONFIG['n_posterior_samples'],\n",
    "            apply_adjustment=use_adjustment,\n",
    "            verbose=False\n",
    "        )\n",
    "        return samples, 0.0  # ABC has no training time\n",
    "\n",
    "    else:\n",
    "        # CNF methods\n",
    "        cnf_info = cnf_samplers[method_name]\n",
    "\n",
    "        # Prepare features using your existing function\n",
    "        x_input = compute_test_input_fixed_optimized_numba(x_obs, method_name)\n",
    "\n",
    "        # Sample from posterior\n",
    "        key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "        samples = cnf_info['cnf'].batch_sample_fn(\n",
    "            CONFIG['n_posterior_samples'], x_input, key\n",
    "        )\n",
    "\n",
    "        return samples, cnf_info['training_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b0d3c",
   "metadata": {
    "id": "dc9b0d3c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import jit, prange\n",
    "from collections import defaultdict\n",
    "\n",
    "@jit(nopython=True)\n",
    "def create_2d_grid_numba():\n",
    "    \"\"\"åˆ›å»º8x8ç½‘æ ¼ç‚¹ - numbaä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    coords = np.linspace(0, 1, 8)\n",
    "    locations = np.zeros((64, 2))\n",
    "    idx = 0\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            locations[idx, 0] = coords[i]\n",
    "            locations[idx, 1] = coords[j]\n",
    "            idx += 1\n",
    "    return locations\n",
    "\n",
    "@jit(nopython=True)\n",
    "def estimate_range_by_correlation_numba(x_obs):\n",
    "    \"\"\"åŸºäº8x8ç›¸é‚»ç‚¹ç›¸å…³æ€§ä¼°è®¡range parameter - numbaä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # è®¡ç®—ç›¸é‚»ç‚¹çš„ç»éªŒç›¸å…³æ€§ä»£ç†\n",
    "    x_mean = np.mean(x_obs)\n",
    "    x_std = np.std(x_obs)\n",
    "    if x_std < 1e-8:\n",
    "        x_std = 1e-8\n",
    "    x_normalized = (x_obs - x_mean) / x_std\n",
    "\n",
    "    # å®šä¹‰ç›¸é‚»ç‚¹å¯¹ - ä½¿ç”¨é™æ€æ•°ç»„\n",
    "    neighbor_pairs = np.array([\n",
    "        # æ°´å¹³ç›¸é‚» (éƒ¨åˆ†)\n",
    "        [0,1], [1,2], [2,3], [8,9], [9,10], [16,17], [24,25],\n",
    "        # å‚ç›´ç›¸é‚» (éƒ¨åˆ†)\n",
    "        [0,8], [1,9], [8,16], [16,24], [24,32], [32,40], [40,48], [48,56],\n",
    "        # å¯¹è§’ç›¸é‚» (éƒ¨åˆ†)\n",
    "        [0,9], [1,10], [8,17], [16,25]\n",
    "    ])\n",
    "\n",
    "    correlations = []\n",
    "    distances = []\n",
    "\n",
    "    for k in range(neighbor_pairs.shape[0]):\n",
    "        i, j = neighbor_pairs[k, 0], neighbor_pairs[k, 1]\n",
    "        if i < 64 and j < 64:\n",
    "            corr = x_normalized[i] * x_normalized[j]\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            dist = np.sqrt(dist_sq)\n",
    "            if corr > 0.01:\n",
    "                correlations.append(corr)\n",
    "                distances.append(dist)\n",
    "\n",
    "    if len(correlations) > 0:\n",
    "        avg_corr = np.mean(np.array(correlations))\n",
    "        avg_dist = np.mean(np.array(distances))\n",
    "\n",
    "        # åæ¨range: corr = exp(-dist/range)\n",
    "        if avg_corr > 0:\n",
    "            range_est = -avg_dist / np.log(avg_corr)\n",
    "            # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…\n",
    "            return min(max(range_est, 5.0), 95.0)\n",
    "\n",
    "    # å¦‚æœç›¸å…³æ€§éƒ½å¤ªå°ï¼Œä½¿ç”¨å…ˆéªŒå‡å€¼\n",
    "    return 50.0\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_distance_groups_numba():\n",
    "    \"\"\"é¢„è®¡ç®—æ‰€æœ‰è·ç¦»åˆ†ç»„ä¿¡æ¯ - numbaä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # è®¡ç®—æ‰€æœ‰è·ç¦»å¹¶æ‰¾åˆ°å”¯ä¸€å€¼\n",
    "    unique_distances = []\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    for i in range(64):\n",
    "        for j in range(i+1, 64):\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            dist = np.sqrt(dist_sq)\n",
    "\n",
    "            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°çš„å”¯ä¸€è·ç¦»\n",
    "            is_unique = True\n",
    "            for existing_dist in unique_distances:\n",
    "                if abs(dist - existing_dist) < tolerance:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                unique_distances.append(dist)\n",
    "\n",
    "    # æ’åºè·ç¦»\n",
    "    unique_distances.sort()\n",
    "    return np.array(unique_distances), locations\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_pairwise_grouped_scores_numba(x_obs, theta_estimated, unique_distances, locations):\n",
    "    \"\"\"è®¡ç®—è·ç¦»åˆ†ç»„çš„pairwise scores - numbaä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    group_scores = np.zeros(len(unique_distances))\n",
    "    sigma = 5.0\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    # å¹¶è¡Œè®¡ç®—æ¯ä¸ªè·ç¦»ç»„çš„åˆ†æ•°\n",
    "    for dist_idx in prange(len(unique_distances)):\n",
    "        target_dist = unique_distances[dist_idx]\n",
    "        group_score_sum = 0.0\n",
    "\n",
    "        # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…è¿™ä¸ªè·ç¦»çš„ç‚¹å¯¹\n",
    "        for i in range(64):\n",
    "            for j in range(i+1, 64):\n",
    "                actual_dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "                actual_dist = np.sqrt(actual_dist_sq)\n",
    "\n",
    "                if abs(actual_dist - target_dist) < tolerance:\n",
    "                    # è®¡ç®—pairwise score\n",
    "                    v_ij = x_obs[i] - x_obs[j]\n",
    "                    exp_term = np.exp(-target_dist / theta_estimated)\n",
    "                    gamma_ij = sigma**2 * (1 - exp_term)\n",
    "                    gamma_ij = max(gamma_ij, 1e-8)\n",
    "                    dgamma_dtheta = sigma**2 * exp_term * target_dist / (theta_estimated**2)\n",
    "                    score_ij = dgamma_dtheta / (2 * gamma_ij) * (v_ij**2 / (2 * gamma_ij) - 1)\n",
    "\n",
    "                    group_score_sum += score_ij\n",
    "\n",
    "        group_scores[dist_idx] = group_score_sum\n",
    "\n",
    "    return group_scores\n",
    "\n",
    "@jit(nopython=True)\n",
    "def normalize_array_numba(arr):\n",
    "    \"\"\"æ•°ç»„æ ‡å‡†åŒ– - numbaä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    mean_val = np.mean(arr)\n",
    "    std_val = np.std(arr)\n",
    "    if std_val < 1e-8:\n",
    "        std_val = 1e-8\n",
    "    return (arr - mean_val) / std_val\n",
    "\n",
    "# é¢„è®¡ç®—é™æ€æ•°æ®ï¼ˆåªéœ€è¦è®¡ç®—ä¸€æ¬¡ï¼‰\n",
    "_unique_distances, _locations = compute_distance_groups_numba()\n",
    "\n",
    "def compute_test_input_fixed_optimized_numba(x_obs, method):\n",
    "    \"\"\"ä¼˜åŒ–ç‰ˆæœ¬çš„æµ‹è¯•è¾“å…¥è®¡ç®—å‡½æ•° - numbaä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    x_obs_array = np.array(x_obs)\n",
    "\n",
    "    if method == 'raw':\n",
    "        return x_obs_array  # 64ç»´åŸå§‹æ•°æ®\n",
    "\n",
    "    # ä¼°è®¡theta (8x8ç‰ˆæœ¬)\n",
    "    theta_estimated = estimate_range_by_correlation_numba(x_obs_array)\n",
    "\n",
    "    # åªè®¡ç®—ä¸€æ¬¡pairwise grouped scores (é¿å…é‡å¤è®¡ç®—2016ä¸ªç‚¹å¯¹!)\n",
    "    pairwise_scores_raw = compute_pairwise_grouped_scores_numba(\n",
    "        x_obs_array, theta_estimated, _unique_distances, _locations\n",
    "    )\n",
    "\n",
    "    if method == 'data_score':\n",
    "        # ç®€å•æ±‚å’Œå¾—åˆ°data score\n",
    "        total_score = np.sum(pairwise_scores_raw)\n",
    "        return np.array([total_score])\n",
    "\n",
    "    elif method == 'pairwise_grouped':\n",
    "        # ç›´æ¥è¿”å›å½’ä¸€åŒ–çš„pairwise scores\n",
    "        normalized_result = normalize_array_numba(pairwise_scores_raw)\n",
    "        return normalized_result\n",
    "\n",
    "    elif method == 'combined_grouped':\n",
    "        # ç»„åˆï¼šdata_score + normalized_pairwise\n",
    "        data_score_part = np.array([np.sum(pairwise_scores_raw)])  # 1ç»´\n",
    "\n",
    "        pairwise_part = normalize_array_numba(pairwise_scores_raw)\n",
    "\n",
    "        # åˆå¹¶å¹¶å½’ä¸€åŒ–\n",
    "        combined_result = np.concatenate((data_score_part, pairwise_part))\n",
    "        combined_result = normalize_array_numba(combined_result)\n",
    "        return combined_result\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded254db",
   "metadata": {
    "id": "ded254db"
   },
   "outputs": [],
   "source": [
    "def compute_test_input_fixed_optimized_numba(x_obs, method):\n",
    "    \"\"\"ä¼˜åŒ–ç‰ˆæœ¬çš„æµ‹è¯•è¾“å…¥è®¡ç®—å‡½æ•° - numbaä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n",
    "    x_obs_array = np.array(x_obs)\n",
    "\n",
    "    if method == 'raw':\n",
    "        return x_obs_array  # 64ç»´åŸå§‹æ•°æ®\n",
    "\n",
    "    # ä¼°è®¡theta (8x8ç‰ˆæœ¬)\n",
    "    theta_estimated = estimate_range_by_correlation_numba(x_obs_array)\n",
    "\n",
    "    # åªè®¡ç®—ä¸€æ¬¡pairwise grouped scores (é¿å…é‡å¤è®¡ç®—2016ä¸ªç‚¹å¯¹!)\n",
    "    pairwise_scores_raw = compute_pairwise_grouped_scores_numba(\n",
    "        x_obs_array, theta_estimated, _unique_distances, _locations\n",
    "    )\n",
    "\n",
    "    if method == 'data_score':\n",
    "        # ç®€å•æ±‚å’Œå¾—åˆ°data score\n",
    "        total_score = np.sum(pairwise_scores_raw)\n",
    "        return np.array([total_score])\n",
    "\n",
    "    elif method == 'pairwise_grouped':\n",
    "        # ç›´æ¥è¿”å›å½’ä¸€åŒ–çš„pairwise scores\n",
    "        normalized_result = normalize_array_numba(pairwise_scores_raw)\n",
    "        return normalized_result\n",
    "\n",
    "    elif method == 'combined_grouped':\n",
    "        # ç»„åˆï¼šdata_score + normalized_pairwise\n",
    "        data_score_part = np.array([np.sum(pairwise_scores_raw)])  # 1ç»´\n",
    "\n",
    "        pairwise_part = normalize_array_numba(pairwise_scores_raw)\n",
    "\n",
    "        # åˆå¹¶å¹¶å½’ä¸€åŒ–\n",
    "        combined_result = np.concatenate((data_score_part, pairwise_part))\n",
    "        combined_result = normalize_array_numba(combined_result)\n",
    "        return combined_result\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c286b",
   "metadata": {
    "id": "3a8c286b",
    "outputId": "c3d9d251-a631-4433-ac9d-b7b271f5fe1b"
   },
   "outputs": [],
   "source": [
    "# åœ¨åŸæœ‰çš„4ç§æ–¹æ³•åŸºç¡€ä¸Šæ·»åŠ ABCä½œä¸ºç¬¬5ç§æ–¹æ³•\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from pyabc import ABCSMC, DistributionBase, Parameter\n",
    "from scipy.stats import uniform\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# é¦–å…ˆè¿è¡ŒABC\n",
    "print(\"ğŸ”¬ è¿è¡ŒABCæ¨æ–­...\")\n",
    "\n",
    "# ä½¿ç”¨ä½ çš„å®Œæ•´ABCå®ç°\n",
    "def gaussian_field_model(params):\n",
    "    \"\"\"Gaussian Fieldæ¨¡æ‹Ÿå™¨ - è¿”å›åŸå§‹æ•°æ®\"\"\"\n",
    "    # æ·»åŠ è°ƒè¯•è®¡æ•°å™¨\n",
    "    if not hasattr(gaussian_field_model, '_call_count'):\n",
    "        gaussian_field_model._call_count = 0\n",
    "        gaussian_field_model._success_count = 0\n",
    "    gaussian_field_model._call_count += 1\n",
    "\n",
    "    # è½¬æ¢å‚æ•°\n",
    "    theta = np.array([params['range_param']])\n",
    "\n",
    "    # éªŒè¯å‚æ•°\n",
    "    if theta[0] <= 0 or theta[0] >= 100:\n",
    "        if gaussian_field_model._call_count <= 5:\n",
    "            print(f\"æ¨¡æ‹Ÿå™¨ #{gaussian_field_model._call_count}: å‚æ•°éªŒè¯å¤±è´¥\")\n",
    "        # è¿”å›ç®€å•çš„æ›¿ä»£æ•°æ®è€Œä¸æ˜¯NaN\n",
    "        return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "    try:\n",
    "        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "        key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "        sim_data = your_simulator(jnp.array(theta), key)  # (64,)\n",
    "\n",
    "        if sim_data is None or np.any(np.isnan(sim_data)) or np.any(np.isinf(sim_data)):\n",
    "            if gaussian_field_model._call_count <= 5:\n",
    "                print(f\"æ¨¡æ‹Ÿå™¨ #{gaussian_field_model._call_count}: æ¨¡æ‹Ÿæ•°æ®æ— æ•ˆ\")\n",
    "            # è¿”å›ç®€å•çš„æ›¿ä»£æ•°æ®è€Œä¸æ˜¯NaN\n",
    "            return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "        # æˆåŠŸç”Ÿæˆæ•°æ®\n",
    "        gaussian_field_model._success_count += 1\n",
    "        if gaussian_field_model._call_count <= 5:\n",
    "            print(f\"æ¨¡æ‹Ÿå™¨ #{gaussian_field_model._call_count}: æˆåŠŸç”Ÿæˆæ•°æ®ï¼Œå½¢çŠ¶={sim_data.shape}\")\n",
    "\n",
    "        # ç¡®ä¿è¿”å›æ­£ç¡®çš„æ•°æ®ç±»å‹ï¼Œå¹¶è¿›è¡Œæ•°å€¼æ£€æŸ¥\n",
    "        sim_array = np.array(sim_data, dtype=np.float64)\n",
    "\n",
    "        # æ›¿æ¢ä»»ä½•å‰©ä½™çš„NaNæˆ–infå€¼\n",
    "        if np.any(np.isnan(sim_array)) or np.any(np.isinf(sim_array)):\n",
    "            print(f\"å‘ç°æ— æ•ˆå€¼ï¼Œä½¿ç”¨æ›¿ä»£æ•°æ®\")\n",
    "            return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "        return {\"data\": sim_array}\n",
    "\n",
    "    except Exception as e:\n",
    "        if gaussian_field_model._call_count <= 5:\n",
    "            print(f\"æ¨¡æ‹Ÿå™¨ #{gaussian_field_model._call_count}: å¼‚å¸¸ - {str(e)}\")\n",
    "        # è¿”å›ç®€å•çš„æ›¿ä»£æ•°æ®è€Œä¸æ˜¯NaN\n",
    "        return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "\n",
    "class GaussianFieldPrior(DistributionBase):\n",
    "    \"\"\"Gaussian Fieldå…ˆéªŒåˆ†å¸ƒ\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # ç®€å•çš„uniformå…ˆéªŒ range ~ uniform(0, 100)\n",
    "        self.range_min = 0.0\n",
    "        self.range_max = 100.0\n",
    "        self.uniform_dist = uniform(loc=self.range_min, scale=self.range_max - self.range_min)\n",
    "\n",
    "        self.param_names = ['range_param']\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"é‡‡æ · - ç®€å•çš„å‡åŒ€åˆ†å¸ƒ\"\"\"\n",
    "        max_attempts = 100\n",
    "        for attempt in range(max_attempts):\n",
    "            # ä»å‡åŒ€åˆ†å¸ƒé‡‡æ ·\n",
    "            raw_sample = self.uniform_dist.rvs()\n",
    "\n",
    "            # éªŒè¯å‚æ•°\n",
    "            if self._validate_sample(raw_sample):\n",
    "                sample_dict = {'range_param': float(raw_sample)}\n",
    "                return Parameter(sample_dict)\n",
    "\n",
    "        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ä¸­é—´å€¼\n",
    "        print(f\"å…ˆéªŒé‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨ä¸­é—´å€¼\")\n",
    "        sample_dict = {'range_param': 50.0}\n",
    "        return Parameter(sample_dict)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        \"\"\"è®¡ç®—å¯†åº¦ - å‡åŒ€åˆ†å¸ƒ\"\"\"\n",
    "        if not isinstance(x, dict):\n",
    "            x = x.to_dict() if hasattr(x, 'to_dict') else dict(x)\n",
    "\n",
    "        # è½¬æ¢ä¸ºæ•°ç»„\n",
    "        theta = x['range_param']\n",
    "\n",
    "        # éªŒè¯å‚æ•°çº¦æŸ\n",
    "        if not self._validate_sample(theta):\n",
    "            return 1e-10\n",
    "\n",
    "        # å‡åŒ€åˆ†å¸ƒå¯†åº¦\n",
    "        try:\n",
    "            density = self.uniform_dist.pdf(theta)\n",
    "            density_val = max(float(density), 1e-10)\n",
    "\n",
    "            # æ£€æŸ¥å¯†åº¦æ˜¯å¦æœ‰æ•ˆ\n",
    "            if np.isnan(density_val) or np.isinf(density_val):\n",
    "                return 1e-10\n",
    "\n",
    "            return density_val\n",
    "        except Exception:\n",
    "            return 1e-10\n",
    "\n",
    "    def _validate_sample(self, theta):\n",
    "        \"\"\"å‚æ•°éªŒè¯\"\"\"\n",
    "        # rangeå‚æ•°èŒƒå›´\n",
    "        if theta <= 0.0 or theta >= 100.0:\n",
    "            return False\n",
    "\n",
    "        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§\n",
    "        if np.isnan(theta) or np.isinf(theta):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "def gaussian_field_distance(x, y):\n",
    "    \"\"\"è·ç¦»å‡½æ•° - ä½¿ç”¨combined_groupedä½œä¸ºæ±‡æ€»ç»Ÿè®¡\"\"\"\n",
    "    data_x = x[\"data\"]  # (64,)\n",
    "    data_y = y[\"data\"]  # (64,)\n",
    "\n",
    "    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§\n",
    "    if np.any(np.isnan(data_x)) or np.any(np.isnan(data_y)):\n",
    "        return 1e8\n",
    "\n",
    "    if np.any(np.isinf(data_x)) or np.any(np.isinf(data_y)):\n",
    "        return 1e8\n",
    "\n",
    "    try:\n",
    "        # ä½¿ç”¨combined_groupedä½œä¸ºsummary statistics\n",
    "        # ç¡®ä¿æ•°ç»„ç±»å‹æ­£ç¡®\n",
    "        data_x = np.asarray(data_x, dtype=np.float64)\n",
    "        data_y = np.asarray(data_y, dtype=np.float64)\n",
    "        summary_x = compute_test_input_fixed_optimized_numba(data_x, 'combined_grouped')\n",
    "        summary_y = compute_test_input_fixed_optimized_numba(data_y, 'combined_grouped')\n",
    "\n",
    "        # æ£€æŸ¥æ±‡æ€»ç»Ÿè®¡æ˜¯å¦æœ‰æ•ˆ\n",
    "        if np.any(np.isnan(summary_x)) or np.any(np.isnan(summary_y)):\n",
    "            return 1e8\n",
    "\n",
    "        if np.any(np.isinf(summary_x)) or np.any(np.isinf(summary_y)):\n",
    "            return 1e8\n",
    "\n",
    "        # æ¬§å¼è·ç¦»æ¯”è¾ƒæ±‡æ€»ç»Ÿè®¡\n",
    "        distance = np.sqrt(np.sum((summary_x - summary_y)**2))\n",
    "\n",
    "        # æ£€æŸ¥è·ç¦»æ˜¯å¦æœ‰æ•ˆ\n",
    "        if np.isnan(distance) or np.isinf(distance) or distance < 0:\n",
    "            return 1e8\n",
    "\n",
    "        return float(distance)\n",
    "\n",
    "    except Exception as e:\n",
    "        return 1e10  # æ˜ç¡®è¡¨ç¤ºè¿™ä¸ªæ ·æœ¬ä¸å¥½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52168c1f",
   "metadata": {
    "id": "52168c1f"
   },
   "outputs": [],
   "source": [
    "# Complete ABC Regression Adjustment Integration Code\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from pyabc import ABCSMC, DistributionBase, Parameter\n",
    "from scipy.stats import uniform, gaussian_kde\n",
    "import tempfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from collections import defaultdict\n",
    "\n",
    "# ========== Helper Functions ==========\n",
    "def _coerce_to_array(item, target_shape):\n",
    "    \"\"\"Convert various formats to numpy array\"\"\"\n",
    "    try:\n",
    "        if hasattr(item, 'numpy'):  # JAX/PyTorch\n",
    "            arr = np.array(item.numpy(), dtype=float)\n",
    "        elif hasattr(item, 'values'):  # pandas\n",
    "            arr = np.array(item.values, dtype=float)\n",
    "        else:\n",
    "            arr = np.array(item, dtype=float)\n",
    "\n",
    "        # Try to reshape to target shape\n",
    "        if arr.shape != target_shape:\n",
    "            if arr.size == np.prod(target_shape):\n",
    "                arr = arr.reshape(target_shape)\n",
    "            else:\n",
    "                return None\n",
    "        return arr\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _epanechnikov_weights(z, bandwidth):\n",
    "    \"\"\"Epanechnikov kernel weights\"\"\"\n",
    "    abs_z = np.abs(z / bandwidth)\n",
    "    w = np.where(abs_z <= 1, 0.75 * (1 - abs_z**2), 0)\n",
    "    return w\n",
    "\n",
    "def _ess(weights):\n",
    "    \"\"\"Effective sample size\"\"\"\n",
    "    w = np.asarray(weights)\n",
    "    w = w[np.isfinite(w) & (w > 0)]\n",
    "    if len(w) == 0:\n",
    "        return 0\n",
    "    return np.sum(w)**2 / np.sum(w**2)\n",
    "\n",
    "def _wls(X, y, weights, ridge=1e-8):\n",
    "    \"\"\"Weighted least squares\"\"\"\n",
    "    W = np.diag(weights)\n",
    "    XtWX = X.T @ W @ X\n",
    "    XtWX += ridge * np.eye(XtWX.shape[0])\n",
    "    XtWy = X.T @ W @ y\n",
    "\n",
    "    try:\n",
    "        beta = np.linalg.solve(XtWX, XtWy)\n",
    "        yhat = X @ beta\n",
    "\n",
    "        # R-squared calculation\n",
    "        y_mean = np.average(y, weights=weights)\n",
    "        ss_tot = np.sum(weights * (y - y_mean)**2)\n",
    "        ss_res = np.sum(weights * (y - yhat)**2)\n",
    "        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "        return beta, yhat, r2\n",
    "    except:\n",
    "        return np.zeros(X.shape[1]), np.zeros(len(y)), 0\n",
    "\n",
    "# ========== Core Regression Adjustment Function ==========\n",
    "def abc_regression_adjustment(\n",
    "    theta_samples,\n",
    "    summary_stats_list,\n",
    "    observed_summary_matrix,\n",
    "    bandwidth_quantile=0.8,\n",
    "    min_effective_n=30,\n",
    "    ridge=0,\n",
    "    feature_clip=6.0,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    ABC regression adjustment - adapted for your code structure\n",
    "    \"\"\"\n",
    "    theta_samples = np.asarray(theta_samples, dtype=float)\n",
    "    n_samples, n_params = theta_samples.shape\n",
    "\n",
    "    obs = np.asarray(observed_summary_matrix, dtype=float)\n",
    "    obs_shape = obs.shape\n",
    "    obs_flat = obs.reshape(-1)\n",
    "    obs_finite_flat = np.isfinite(obs_flat)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ABC regression adjustment: {n_samples} samples, feature dimension {obs_shape}\")\n",
    "\n",
    "    # ========== Calculate distances ==========\n",
    "    dists = np.full(n_samples, np.inf, dtype=float)\n",
    "    coerced_list = [None] * n_samples\n",
    "    converted, failed = 0, 0\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        if k >= len(summary_stats_list):\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        sim = _coerce_to_array(summary_stats_list[k], obs_shape)\n",
    "        if sim is None:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        coerced_list[k] = sim\n",
    "        sim_flat = sim.reshape(-1)\n",
    "        both = obs_finite_flat & np.isfinite(sim_flat)\n",
    "        if not np.any(both):\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        diff = (sim_flat - obs_flat)[both]\n",
    "        dists[k] = np.sqrt(np.sum(diff * diff))\n",
    "        converted += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Distance calculation: succeeded={converted}, failed={failed}\")\n",
    "\n",
    "    valid = np.isfinite(dists)\n",
    "    if not np.any(valid):\n",
    "        if verbose:\n",
    "            print(\"Error: no valid distance calculations\")\n",
    "        return theta_samples.copy()\n",
    "\n",
    "    # ========== Standardize distances and bandwidth ==========\n",
    "    z = dists[valid] - dists[valid].mean()\n",
    "    std = dists[valid].std()\n",
    "    z = z / std if std > 0 else np.zeros_like(z)\n",
    "\n",
    "    bandwidth = np.quantile(np.abs(z), bandwidth_quantile)\n",
    "    if not np.isfinite(bandwidth) or bandwidth <= 0:\n",
    "        bandwidth = 1.0\n",
    "\n",
    "    z_all = np.full_like(dists, np.nan, dtype=float)\n",
    "    z_all[valid] = (dists[valid] - dists[valid].mean()) / (std if std > 0 else 1.0)\n",
    "\n",
    "    # ========== Kernel weights ==========\n",
    "    w = _epanechnikov_weights(z_all, bandwidth)\n",
    "    keep_idx = np.where(w > 0)[0]\n",
    "    ess = _ess(w)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Bandwidth: {bandwidth:.6f}, effective samples: {keep_idx.size}, ESS: {ess:.1f}\")\n",
    "\n",
    "    if keep_idx.size < 5:\n",
    "        if verbose:\n",
    "            print(\"Too few effective samples, returning original samples\")\n",
    "        return theta_samples.copy()\n",
    "\n",
    "    # ========== Construct design matrix ==========\n",
    "    mask = np.isfinite(obs_flat)\n",
    "    p = int(mask.sum())\n",
    "\n",
    "    X_rows = []\n",
    "    kept_ok = []\n",
    "    for k in keep_idx:\n",
    "        sim = coerced_list[k]\n",
    "        if sim is None:\n",
    "            continue\n",
    "        sim2_flat = sim.reshape(-1).copy()\n",
    "        bad = ~np.isfinite(sim2_flat)\n",
    "        if np.any(bad):\n",
    "            sim2_flat[bad] = obs_flat[bad]\n",
    "        diff_vec = (sim2_flat - obs_flat)[mask]\n",
    "        X_rows.append(diff_vec)\n",
    "        kept_ok.append(k)\n",
    "\n",
    "    if len(X_rows) < 5:\n",
    "        if verbose:\n",
    "            print(\"Insufficient samples after design matrix construction\")\n",
    "        return theta_samples.copy()\n",
    "\n",
    "    kept_ok = np.asarray(kept_ok, dtype=int)\n",
    "    X = np.asarray(X_rows)\n",
    "\n",
    "    # Column standardization\n",
    "    col_mean = X.mean(axis=0)\n",
    "    col_std = X.std(axis=0, ddof=0)\n",
    "    col_std[col_std == 0] = 1.0\n",
    "    Xs = (X - col_mean) / col_std\n",
    "\n",
    "    if feature_clip is not None and feature_clip > 0:\n",
    "        np.clip(Xs, -feature_clip, feature_clip, out=Xs)\n",
    "\n",
    "    # Add intercept\n",
    "    Xs = np.column_stack([np.ones(Xs.shape[0]), Xs])\n",
    "\n",
    "    # ========== Parameter-wise regression adjustment ==========\n",
    "    theta_adj = theta_samples.copy()\n",
    "\n",
    "    for pidx in range(n_params):\n",
    "        if verbose:\n",
    "            print(f\"Adjusting parameter {pidx+1}/{n_params}\")\n",
    "\n",
    "        y = theta_samples[kept_ok, pidx]\n",
    "        beta, yhat, r2 = _wls(Xs, y, w[kept_ok], ridge=ridge)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  RÂ²: {r2:.4f}\")\n",
    "\n",
    "        # Only adjust samples with kernel weights\n",
    "        for k in kept_ok:\n",
    "            sim = coerced_list[k]\n",
    "            if sim is None:\n",
    "                continue\n",
    "            sim2_flat = sim.reshape(-1).copy()\n",
    "            bad = ~np.isfinite(sim2_flat)\n",
    "            if np.any(bad):\n",
    "                sim2_flat[bad] = obs_flat[bad]\n",
    "            diff_vec = (sim2_flat - obs_flat)[mask]\n",
    "            xk = (diff_vec - col_mean) / col_std\n",
    "            if feature_clip is not None and feature_clip > 0:\n",
    "                np.clip(xk, -feature_clip, feature_clip, out=xk)\n",
    "            xk = np.concatenate([[1.0], xk])\n",
    "            yk_pred = xk @ beta\n",
    "            adjustment = yk_pred - beta[0]\n",
    "            theta_adj[k, pidx] = theta_samples[k, pidx] - adjustment\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Regression adjustment complete!\")\n",
    "\n",
    "    return theta_adj\n",
    "\n",
    "# ========== Enhanced ABC Implementation ==========\n",
    "def run_abc_inference_with_adjustment(x_obs, n_samples, apply_adjustment=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Enhanced ABC inference with regression adjustment\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Starting ABC inference (samples: {n_samples}, regression adjustment: {apply_adjustment})\")\n",
    "\n",
    "    # Reset simulator counters\n",
    "    if hasattr(gaussian_field_model, '_call_count'):\n",
    "        gaussian_field_model._call_count = 0\n",
    "        gaussian_field_model._success_count = 0\n",
    "\n",
    "    # ABC setup\n",
    "    prior = GaussianFieldPrior()\n",
    "\n",
    "    abc = ABCSMC(\n",
    "        models=gaussian_field_model,\n",
    "        parameter_priors=prior,\n",
    "        distance_function=gaussian_field_distance,\n",
    "        population_size= n_samples,\n",
    "        sampler=pyabc.sampler.SingleCoreSampler(),\n",
    "        eps=pyabc.epsilon.QuantileEpsilon(alpha=0.5, quantile_multiplier=0.95)\n",
    "    )\n",
    "\n",
    "    # Prepare observed data\n",
    "    obs = {\"data\": np.array(x_obs, dtype=np.float64)}\n",
    "\n",
    "    # Create database\n",
    "    db_path = os.path.join(tempfile.gettempdir(), f\"abc_adjusted_{int(time.time())}.db\")\n",
    "    abc.new(\"sqlite:///\" + db_path, obs)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Running ABC sampling...\")\n",
    "\n",
    "    try:\n",
    "        # Run ABC\n",
    "        history = abc.run(max_nr_populations=3)\n",
    "\n",
    "        # Extract samples\n",
    "        df, w = history.get_distribution(m=0, t=history.max_t)\n",
    "        theta_samples = df['range_param'].values\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ABC complete: {len(theta_samples)} samples\")\n",
    "\n",
    "        # ========== Apply regression adjustment ==========\n",
    "        if apply_adjustment and len(theta_samples) > 50:\n",
    "            if verbose:\n",
    "                print(\"Starting regression adjustment...\")\n",
    "\n",
    "            # Recompute summary statistics for regression adjustment\n",
    "            summary_stats_list = []\n",
    "            obs_summary = compute_test_input_fixed_optimized_numba(x_obs, 'combined_grouped')\n",
    "\n",
    "            # Recompute summary statistics for each ABC sample\n",
    "            n_for_adjustment = min(len(theta_samples), 500)  # Limit number to avoid slowdown\n",
    "            for i in range(n_for_adjustment):\n",
    "                theta_val = theta_samples[i]\n",
    "                if verbose and i % 100 == 0:\n",
    "                    print(f\"  Recomputing summary statistics {i+1}/{n_for_adjustment}\")\n",
    "\n",
    "                try:\n",
    "                    # Simulate data with this theta value\n",
    "                    key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "                    sim_data = your_simulator(jnp.array([theta_val]), key)\n",
    "                    summary_stats = compute_test_input_fixed_optimized_numba(sim_data, 'combined_grouped')\n",
    "                    summary_stats_list.append(summary_stats)\n",
    "                except Exception as e:\n",
    "                    if verbose and i < 5:\n",
    "                        print(f\"    Sample {i} simulation failed: {e}\")\n",
    "                    # Use observed values when failed\n",
    "                    summary_stats_list.append(obs_summary.copy())\n",
    "\n",
    "            # Only adjust samples for which we recomputed summary statistics\n",
    "            theta_for_adjustment = theta_samples[:n_for_adjustment].reshape(-1, 1)\n",
    "\n",
    "            # Apply regression adjustment\n",
    "            theta_adjusted = abc_regression_adjustment(\n",
    "                theta_samples=theta_for_adjustment,\n",
    "                summary_stats_list=summary_stats_list,\n",
    "                observed_summary_matrix=obs_summary,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            # Combine adjusted and unadjusted samples\n",
    "            final_samples = np.concatenate([\n",
    "                theta_adjusted.flatten(),\n",
    "                theta_samples[n_for_adjustment:]\n",
    "            ])\n",
    "\n",
    "            if verbose:\n",
    "                orig_mean = np.mean(theta_samples)\n",
    "                adj_mean = np.mean(final_samples)\n",
    "                print(f\"Adjustment effect: original mean={orig_mean:.3f}, adjusted mean={adj_mean:.3f}\")\n",
    "        else:\n",
    "            final_samples = theta_samples\n",
    "            if verbose and apply_adjustment:\n",
    "                print(\"Insufficient samples, skipping regression adjustment\")\n",
    "\n",
    "        # Randomly sample specified number if needed\n",
    "        if len(final_samples) > n_samples:\n",
    "            indices = np.random.choice(len(final_samples), n_samples, replace=False)\n",
    "            final_samples = final_samples[indices]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Returning {len(final_samples)} samples\")\n",
    "\n",
    "        # Clean up temporary database\n",
    "        try:\n",
    "            os.remove(db_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return jnp.array(final_samples)\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"ABC failed: {str(e)}\")\n",
    "        # Clean up temporary database\n",
    "        try:\n",
    "            os.remove(db_path)\n",
    "        except:\n",
    "            pass\n",
    "        return jr.uniform(jr.PRNGKey(42), (n_samples,), minval=5.0, maxval=95.0)\n",
    "\n",
    "# ========== Update original ABC function for compatibility ==========\n",
    "def run_abc_inference(x_obs, n_samples=1000, use_adjustment=False):\n",
    "    \"\"\"\n",
    "    Updated ABC inference function - compatible with MSE experiments\n",
    "    \"\"\"\n",
    "    return run_abc_inference_with_adjustment(\n",
    "        x_obs, n_samples=n_samples,\n",
    "        apply_adjustment=use_adjustment,\n",
    "        verbose=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4982e",
   "metadata": {
    "id": "87a4982e"
   },
   "outputs": [],
   "source": [
    "def run_single_inference(method_name, x_obs, cnf_samplers=None):\n",
    "    \"\"\"Run inference for a single method\"\"\"\n",
    "\n",
    "    start_time = time.time()  # å¼€å§‹è®¡æ—¶\n",
    "\n",
    "    if method_name in ['abc', 'abc_adjusted']:\n",
    "        use_adjustment = (method_name == 'abc_adjusted')\n",
    "        samples = run_abc_inference_with_adjustment(\n",
    "            x_obs,\n",
    "            n_samples=CONFIG['n_posterior_samples'],\n",
    "            apply_adjustment=use_adjustment,\n",
    "            verbose=False\n",
    "        )\n",
    "        inference_time = time.time() - start_time  # è®¡ç®—æ¨ç†æ—¶é—´\n",
    "        return samples, inference_time, 0.0, CONFIG['n_posterior_samples']\n",
    "        # è¿”å›ï¼šæ ·æœ¬ï¼Œæ¨ç†æ—¶é—´ï¼Œè®­ç»ƒæ—¶é—´(0)ï¼Œè®­ç»ƒæ ·æœ¬æ•°(å°±æ˜¯åéªŒæ ·æœ¬æ•°)\n",
    "\n",
    "    else:\n",
    "        cnf_info = cnf_samplers[method_name]\n",
    "        x_input = compute_test_input_fixed_optimized_numba(x_obs, method_name)\n",
    "\n",
    "        key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "        samples = cnf_info['cnf'].batch_sample_fn(\n",
    "            CONFIG['n_posterior_samples'], x_input, key\n",
    "        )\n",
    "\n",
    "        inference_time = time.time() - start_time  # è®¡ç®—æ¨ç†æ—¶é—´\n",
    "        return samples, inference_time, cnf_info['training_time'], CONFIG['n_training_samples']\n",
    "        # è¿”å›ï¼šæ ·æœ¬ï¼Œæ¨ç†æ—¶é—´ï¼Œè®­ç»ƒæ—¶é—´ï¼Œè®­ç»ƒæ ·æœ¬æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93607017",
   "metadata": {
    "id": "93607017",
    "outputId": "9d6a8cce-3dd6-4d1e-f744-ab5b4d61f836"
   },
   "outputs": [],
   "source": [
    "def run_full_experiment():\n",
    "    \"\"\"Run the complete comparison experiment\"\"\"\n",
    "    print(\"Starting full comparison experiment...\")\n",
    "    print(f\"Testing theta values: {CONFIG['test_theta_values']}\")\n",
    "    print(f\"Methods: {CONFIG['methods_to_test']}\")\n",
    "\n",
    "    results = {method: {\n",
    "    'estimates': [],\n",
    "    'inference_times': [],    # æ”¹åï¼šåªå­˜æ¨ç†æ—¶é—´\n",
    "    'training_time': 0,       # æ€»è®­ç»ƒæ—¶é—´\n",
    "    'n_training_samples': 0   # æ–°å¢ï¼šè®­ç»ƒæ ·æœ¬æ•°\n",
    "} for method in CONFIG['methods_to_test']}\n",
    "\n",
    "    total_experiments = len(CONFIG['test_theta_values']) * CONFIG['n_experiments_per_theta']\n",
    "    experiment_count = 0\n",
    "\n",
    "    for theta_idx, true_theta in enumerate(CONFIG['test_theta_values']):\n",
    "        print(f\"\\nTesting theta = {true_theta} ({theta_idx+1}/{len(CONFIG['test_theta_values'])})\")\n",
    "\n",
    "        for exp_idx in range(CONFIG['n_experiments_per_theta']):\n",
    "            experiment_count += 1\n",
    "\n",
    "            if CONFIG['verbose'] and exp_idx % 5 == 0:\n",
    "                print(f\"  Experiment {exp_idx+1}/{CONFIG['n_experiments_per_theta']}\")\n",
    "\n",
    "            # Generate test observation\n",
    "            key = jr.PRNGKey(CONFIG['RANDOM_SEED'] + experiment_count)\n",
    "            theta_array = jnp.array([float(true_theta)])\n",
    "            x_obs = your_simulator(theta_array, key)\n",
    "\n",
    "            # Test each method\n",
    "            for method_name in CONFIG['methods_to_test']:\n",
    "                try:\n",
    "                    # æ¥æ”¶4ä¸ªè¿”å›å€¼\n",
    "                    samples, inference_time, training_time, n_training_samples = run_single_inference(\n",
    "                        method_name, x_obs, cnf_samplers\n",
    "                    )\n",
    "\n",
    "                    # å­˜å‚¨ç»“æœ\n",
    "                    results[method_name]['inference_times'].append(inference_time)\n",
    "                    results[method_name]['training_time'] = training_time\n",
    "                    results[method_name]['n_training_samples'] = n_training_samples\n",
    "\n",
    "                    # Store results\n",
    "                    posterior_mean = float(jnp.mean(samples))\n",
    "                    results[method_name]['estimates'].append({\n",
    "                        'true_theta': true_theta,\n",
    "                        'estimate': posterior_mean,\n",
    "                        'samples': samples\n",
    "                    })\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    {method_name} failed: {str(e)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "experiment_results = run_full_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b9411",
   "metadata": {
    "id": "258b9411",
    "outputId": "3f9f184e-9630-487f-998d-bb6ee0a31dc0"
   },
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze and display results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Calculate MSE and timing for each method\n",
    "    mse_results = {}\n",
    "    timing_results = {}\n",
    "\n",
    "    for method_name, method_results in results.items():\n",
    "        estimates_data = method_results['estimates']\n",
    "\n",
    "        # è®¡ç®—è®­ç»ƒæˆæœ¬åˆ†æ‘Š\n",
    "        training_time = method_results['training_time']\n",
    "        n_training_samples = method_results['n_training_samples']\n",
    "        if n_training_samples > 0:\n",
    "            training_cost_per_sample = training_time / n_training_samples\n",
    "        else:\n",
    "            training_cost_per_sample = 0\n",
    "\n",
    "        # Group by theta value\n",
    "        theta_groups = {}\n",
    "        theta_times = {}\n",
    "\n",
    "        for i, est in enumerate(estimates_data):\n",
    "            theta = est['true_theta']\n",
    "            if theta not in theta_groups:\n",
    "                theta_groups[theta] = []\n",
    "                theta_times[theta] = []\n",
    "            theta_groups[theta].append(est['estimate'])\n",
    "            theta_times[theta].append(method_results['inference_times'][i])\n",
    "\n",
    "        # Calculate MSE and costs for each theta\n",
    "        method_mse = {}\n",
    "        method_total_costs = {}\n",
    "        for theta in theta_groups.keys():\n",
    "            estimates = theta_groups[theta]\n",
    "            mse = np.mean([(est - theta)**2 for est in estimates])\n",
    "            method_mse[theta] = mse\n",
    "\n",
    "            inference_time = np.mean(theta_times[theta])\n",
    "            total_cost = training_cost_per_sample + inference_time\n",
    "            method_total_costs[theta] = total_cost\n",
    "\n",
    "        mse_results[method_name] = method_mse\n",
    "        timing_results[method_name] = method_total_costs\n",
    "\n",
    "        # Print results for each theta\n",
    "        print(f\"\\n{method_name.upper()}:\")\n",
    "        for theta in sorted(method_mse.keys()):\n",
    "            inference_time = np.mean(theta_times[theta])\n",
    "            print(f\"  Î¸={theta}: MSE={method_mse[theta]:.4f}\")\n",
    "            print(f\"           Training Cost={training_cost_per_sample:.6f}s, Inference Cost={inference_time:.6f}s, Total Cost={method_total_costs[theta]:.6f}s\")\n",
    "\n",
    "    # è®¡ç®—ç›¸å¯¹æ•ˆç‡\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RELATIVE EFFICIENCY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # æ‰¾åˆ°åŸºå‡†æ–¹æ³• (é€šå¸¸é€‰æ‹©ä¸€ä¸ªä½œä¸ºå‚è€ƒ)\n",
    "    baseline_method = list(mse_results.keys())[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–¹æ³•ä½œä¸ºåŸºå‡†\n",
    "\n",
    "    for theta in sorted(mse_results[baseline_method].keys()):\n",
    "        print(f\"\\nÎ¸={theta}:\")\n",
    "        baseline_mse = mse_results[baseline_method][theta]\n",
    "        baseline_cost = timing_results[baseline_method][theta]\n",
    "\n",
    "        for method_name in mse_results.keys():\n",
    "            method_mse = mse_results[method_name][theta]\n",
    "            method_cost = timing_results[method_name][theta]\n",
    "\n",
    "            # ç›¸å¯¹æ•ˆç‡ = (baseline_mse / method_mse) / (method_cost / baseline_cost)\n",
    "            # = (baseline_mse / method_mse) * (baseline_cost / method_cost)\n",
    "            relative_efficiency = (baseline_mse / method_mse) * (baseline_cost / method_cost)\n",
    "\n",
    "            print(f\"  {method_name}: Relative Efficiency = {relative_efficiency:.3f}\")\n",
    "\n",
    "    return mse_results, timing_results\n",
    "\n",
    "# ä¿®æ”¹ç»˜å›¾å‡½æ•°\n",
    "def plot_comparison_results(mse_results, timing_results):\n",
    "    \"\"\"Plot comparison results\"\"\"\n",
    "    if not CONFIG['plot_results']:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # MSE plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for method_name, method_mse in mse_results.items():\n",
    "        thetas = sorted(method_mse.keys())\n",
    "        mses = [method_mse[theta] for theta in thetas]\n",
    "        plt.plot(thetas, mses, 'o-', label=method_name, linewidth=2)\n",
    "\n",
    "    plt.xlabel('True Î¸')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Mean Squared Error vs True Î¸')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Cost plot - æ˜¾ç¤ºå¹³å‡æˆæœ¬\n",
    "    plt.subplot(1, 2, 2)\n",
    "    methods = list(timing_results.keys())\n",
    "    # è®¡ç®—æ¯ä¸ªæ–¹æ³•çš„å¹³å‡æˆæœ¬\n",
    "    avg_costs = [np.mean(list(timing_results[method].values())) for method in methods]\n",
    "\n",
    "    plt.bar(methods, avg_costs)\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Average Total Cost per Sample (s)')\n",
    "    plt.title('Average Computational Cost Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# è°ƒç”¨\n",
    "mse_results, cost_results = analyze_results(experiment_results)\n",
    "plot_comparison_results(mse_results, cost_results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
