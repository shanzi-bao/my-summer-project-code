{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36203404",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1757541506892,
     "user": {
      "displayName": "Shanzi Bao",
      "userId": "16567399982080387951"
     },
     "user_tz": -480
    },
    "id": "36203404",
    "outputId": "e63d1382-a8da-429a-dae2-728f273795e8"
   },
   "outputs": [],
   "source": [
    "# All your existing imports\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from config import create_range_parameter_config\n",
    "from dataset import your_prior_sampler, your_simulator\n",
    "from nn_model import NCMLP\n",
    "from sde import get_sde\n",
    "from train import train_score_network\n",
    "from cnf import CNF\n",
    "\n",
    "# MAIN CONFIGURATION - Single place to control everything\n",
    "CONFIG = {\n",
    "    'EXPERIMENT_MODE': 'quick',  # 'quick' or 'full'\n",
    "    'RANDOM_SEED': 42,\n",
    "\n",
    "    # Training data\n",
    "    'n_training_samples': 100000,  # 1000 for quick, 5000 for full\n",
    "\n",
    "    # Test parameters\n",
    "    'test_theta_values': [15, 45, 75],  # Will expand for full mode\n",
    "    'n_experiments_per_theta': 5,       # 5 for quick, 20 for full\n",
    "    'n_posterior_samples': 10000,\n",
    "\n",
    "    # Methods to compare\n",
    "    'methods_to_test': ['raw', 'data_score', 'pairwise_grouped', 'combined_grouped', 'abc', 'abc_adjusted'],\n",
    "\n",
    "    # Output settings\n",
    "    'save_results': True,\n",
    "    'plot_results': True,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "# Auto-expand for full experiment\n",
    "if CONFIG['EXPERIMENT_MODE'] == 'full':\n",
    "    CONFIG.update({\n",
    "        'n_training_samples': 100000,\n",
    "        'test_theta_values': list(range(5, 100, 10)),\n",
    "        'n_experiments_per_theta': 20\n",
    "    })\n",
    "\n",
    "print(f\"Running {CONFIG['EXPERIMENT_MODE']} experiment\")\n",
    "print(f\"Testing methods: {CONFIG['methods_to_test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f376e8",
   "metadata": {
    "id": "b6f376e8"
   },
   "outputs": [],
   "source": [
    "def estimate_range_by_correlation(x_obs):\n",
    "    \"\"\"Estimate range parameter based on spatial correlation of neighboring points in 8x8 grid\"\"\"\n",
    "    locations = create_2d_grid()  # (64, 2) - Grid coordinates\n",
    "\n",
    "    # Compute empirical correlation proxy for neighboring points\n",
    "    x_normalized = (x_obs - jnp.mean(x_obs)) / (jnp.std(x_obs) + 1e-8)\n",
    "\n",
    "    # Use multiple neighbor pairs for correlation estimation - adapted for 8x8 grid\n",
    "    neighbor_pairs = [\n",
    "        # Horizontal neighbors (within same row)\n",
    "        (0,1), (1,2), (2,3), (3,4), (4,5), (5,6), (6,7),  # Row 1\n",
    "        (8,9), (9,10), (10,11), (11,12), (12,13), (13,14), (14,15),  # Row 2\n",
    "        (16,17), (24,25), (32,33), (40,41), (48,49), (56,57),  # Sampled from other rows\n",
    "        # Vertical neighbors (within same column)\n",
    "        (0,8), (1,9), (2,10), (3,11), (4,12), (5,13), (6,14), (7,15),  # Rows 1-2\n",
    "        (8,16), (9,17), (16,24), (17,25), (24,32), (32,40), (40,48), (48,56),  # Sampled from other columns\n",
    "        # Diagonal neighbors\n",
    "        (0,9), (1,10), (2,11), (8,17), (9,18), (16,25), (24,33)\n",
    "    ]\n",
    "\n",
    "    correlations = []\n",
    "    distances = []\n",
    "\n",
    "    for i, j in neighbor_pairs:\n",
    "        if i < 64 and j < 64:  # Ensure indices are within 64-point range\n",
    "            corr = x_normalized[i] * x_normalized[j]\n",
    "            dist = jnp.sqrt(jnp.sum((locations[i] - locations[j])**2))  # 2D Euclidean distance\n",
    "            if corr > 0.01:\n",
    "                correlations.append(corr)\n",
    "                distances.append(dist)\n",
    "\n",
    "    if correlations:\n",
    "        avg_corr = jnp.mean(jnp.array(correlations))\n",
    "        avg_dist = jnp.mean(jnp.array(distances))\n",
    "\n",
    "        # Reverse-estimate range: corr = exp(-dist/range)\n",
    "        range_est = -avg_dist / jnp.log(avg_corr)\n",
    "\n",
    "        # Constrain to reasonable range\n",
    "        return float(jnp.clip(range_est, 5.0, 95.0))\n",
    "\n",
    "    # If correlations are too small, use prior mean\n",
    "    return 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591f439",
   "metadata": {
    "id": "7591f439"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "from collections import defaultdict\n",
    "\n",
    "@jit(nopython=True)\n",
    "def create_2d_grid_numba():\n",
    "    \"\"\"Create 8x8 grid points - numba optimized version\"\"\"\n",
    "    coords = np.linspace(0, 1, 8)\n",
    "    locations = np.zeros((64, 2))\n",
    "    idx = 0\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            locations[idx, 0] = coords[i]\n",
    "            locations[idx, 1] = coords[j]\n",
    "            idx += 1\n",
    "    return locations\n",
    "\n",
    "@jit(nopython=True)\n",
    "def estimate_range_by_correlation_numba(x_obs):\n",
    "    \"\"\"Estimate range parameter based on 8x8 neighboring points correlation - numba optimized version\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # Calculate empirical correlation proxy for neighboring points\n",
    "    x_mean = np.mean(x_obs)\n",
    "    x_std = np.std(x_obs)\n",
    "    if x_std < 1e-8:\n",
    "        x_std = 1e-8\n",
    "    x_normalized = (x_obs - x_mean) / x_std\n",
    "\n",
    "    # Define neighboring point pairs - use static array instead of list comprehension\n",
    "    neighbor_pairs = np.array([\n",
    "        # Horizontal neighbors\n",
    "        [0,1], [1,2], [2,3], [3,4], [4,5], [5,6], [6,7],\n",
    "        [8,9], [9,10], [10,11], [11,12], [12,13], [13,14], [14,15],\n",
    "        [16,17], [17,18], [18,19], [19,20], [20,21], [21,22], [22,23],\n",
    "        [24,25], [25,26], [26,27], [27,28], [28,29], [29,30], [30,31],\n",
    "        [32,33], [33,34], [34,35], [35,36], [36,37], [37,38], [38,39],\n",
    "        [40,41], [41,42], [42,43], [43,44], [44,45], [45,46], [46,47],\n",
    "        [48,49], [49,50], [50,51], [51,52], [52,53], [53,54], [54,55],\n",
    "        [56,57], [57,58], [58,59], [59,60], [60,61], [61,62], [62,63],\n",
    "        # Vertical neighbors\n",
    "        [0,8], [1,9], [2,10], [3,11], [4,12], [5,13], [6,14], [7,15],\n",
    "        [8,16], [9,17], [10,18], [11,19], [12,20], [13,21], [14,22], [15,23],\n",
    "        [16,24], [17,25], [18,26], [19,27], [20,28], [21,29], [22,30], [23,31],\n",
    "        [24,32], [25,33], [26,34], [27,35], [28,36], [29,37], [30,38], [31,39],\n",
    "        [32,40], [33,41], [34,42], [35,43], [36,44], [37,45], [38,46], [39,47],\n",
    "        [40,48], [41,49], [42,50], [43,51], [44,52], [45,53], [46,54], [47,55],\n",
    "        [48,56], [49,57], [50,58], [51,59], [52,60], [53,61], [54,62], [55,63],\n",
    "        # Diagonal neighbors (partial)\n",
    "        [0,9], [1,10], [2,11], [3,12], [4,13], [5,14], [6,15],\n",
    "        [8,17], [9,18], [10,19], [11,20], [12,21], [13,22], [14,23],\n",
    "        [16,25], [17,26], [18,27], [19,28], [20,29], [21,30], [22,31]\n",
    "    ])\n",
    "\n",
    "    correlations = []\n",
    "    distances = []\n",
    "\n",
    "    for k in range(neighbor_pairs.shape[0]):\n",
    "        i, j = neighbor_pairs[k, 0], neighbor_pairs[k, 1]\n",
    "        if i < 64 and j < 64:\n",
    "            corr = x_normalized[i] * x_normalized[j]\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            dist = np.sqrt(dist_sq)\n",
    "            if corr > 0.01:\n",
    "                correlations.append(corr)\n",
    "                distances.append(dist)\n",
    "\n",
    "    if len(correlations) > 0:\n",
    "        avg_corr = np.mean(np.array(correlations))\n",
    "        avg_dist = np.mean(np.array(distances))\n",
    "\n",
    "        # Reverse-estimate range: corr = exp(-dist/range)\n",
    "        if avg_corr > 0:\n",
    "            range_est = -avg_dist / np.log(avg_corr)\n",
    "            # Constrain to reasonable range\n",
    "            return min(max(range_est, 5.0), 95.0)\n",
    "\n",
    "    # If correlations are too small, use prior mean\n",
    "    return 50.0\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_all_distances_and_groups_numba():\n",
    "    \"\"\"Calculate all distances and create groups - numba optimized version\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # Pre-compute all distances\n",
    "    distances = np.zeros((64, 64))\n",
    "    for i in range(64):\n",
    "        for j in range(64):\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            distances[i, j] = np.sqrt(dist_sq)\n",
    "\n",
    "    # Find unique distances (using simple deduplication logic)\n",
    "    unique_distances = []\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    for i in range(64):\n",
    "        for j in range(i+1, 64):\n",
    "            dist = distances[i, j]\n",
    "            is_unique = True\n",
    "            for existing_dist in unique_distances:\n",
    "                if abs(dist - existing_dist) < tolerance:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                unique_distances.append(dist)\n",
    "\n",
    "    # Sort distances\n",
    "    unique_distances.sort()\n",
    "\n",
    "    return distances, np.array(unique_distances)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_grouped_scores_for_sample_numba(x, distances, unique_distances, range_param, sigma=5.0):\n",
    "    \"\"\"Calculate grouped scores for single sample - numba optimized version\"\"\"\n",
    "    group_scores = np.zeros(len(unique_distances))\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    for dist_idx in prange(len(unique_distances)):\n",
    "        target_dist = unique_distances[dist_idx]\n",
    "        group_score_sum = 0.0\n",
    "\n",
    "        # Find all point pairs matching this distance\n",
    "        for i in range(64):\n",
    "            for j in range(i+1, 64):\n",
    "                actual_dist = distances[i, j]\n",
    "                if abs(actual_dist - target_dist) < tolerance:\n",
    "                    # Calculate pairwise difference\n",
    "                    v_ij = x[i] - x[j]\n",
    "\n",
    "                    # Calculate semivariogram γ(d; θ) = σ²(1 - exp(-d/θ))\n",
    "                    exp_term = np.exp(-target_dist / range_param)\n",
    "                    gamma_ij = sigma**2 * (1 - exp_term)\n",
    "                    gamma_ij = max(gamma_ij, 1e-8)  # Avoid numerical issues\n",
    "\n",
    "                    # Calculate derivative of γ with respect to range_param\n",
    "                    dgamma_dtheta = sigma**2 * exp_term * target_dist / (range_param**2)\n",
    "\n",
    "                    # Calculate pairwise score (based on paper formula 5.8)\n",
    "                    score_ij = dgamma_dtheta / (2 * gamma_ij) * (v_ij**2 / (2 * gamma_ij) - 1)\n",
    "\n",
    "                    group_score_sum += score_ij\n",
    "\n",
    "        group_scores[dist_idx] = group_score_sum\n",
    "\n",
    "    return group_scores\n",
    "\n",
    "def compute_distance_grouped_pairwise_scores_estimated_theta_numba(x_samples, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate distance-grouped pairwise scores using estimated theta (based on pairwise differences) - numba optimized 64-point version\n",
    "    \"\"\"\n",
    "    print(\"🔄 Calculating distance-grouped pairwise scores using estimated theta (based on pairwise differences, 64 points, numba optimized)...\")\n",
    "\n",
    "    # Pre-compute distance information\n",
    "    distances, unique_distances = compute_all_distances_and_groups_numba()\n",
    "\n",
    "    # Calculate grouped scores for each sample\n",
    "    all_grouped_scores = []\n",
    "    sigma = 5.0  # Standard deviation of spatial process\n",
    "\n",
    "    for sample_idx in range(len(x_samples)):\n",
    "        x = np.array(x_samples[sample_idx])\n",
    "\n",
    "        # Estimate theta instead of using true theta\n",
    "        range_param = estimate_range_by_correlation_numba(x)\n",
    "\n",
    "        # Calculate grouped scores for this sample\n",
    "        group_scores = compute_grouped_scores_for_sample_numba(x, distances, unique_distances, range_param, sigma)\n",
    "\n",
    "        all_grouped_scores.append(group_scores)\n",
    "\n",
    "    result = np.array(all_grouped_scores)\n",
    "    print(f\"✅ 64-point version grouped pairwise scores calculation completed, output dimensions: {result.shape}\")\n",
    "    print(f\"   {len(unique_distances)} dimensions correspond to aggregated scores of point pairs at different 8x8 distances\")\n",
    "    print(f\"   Distance groups include: {[round(d, 3) for d in unique_distances]}\")\n",
    "\n",
    "    return result, unique_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5ffbf",
   "metadata": {
    "id": "f4d5ffbf"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Optimized version - compute pairwise first, then sum to get data score\n",
    "def prepare_all_data_correlated_simplified_optimized(n_samples, key=42):\n",
    "    \"\"\"Prepare correlated data (optimized: compute pairwise first, then sum) - 64-point version\"\"\"\n",
    "    print(\"Preparing optimized correlated data comparison (64 points)...\")\n",
    "    key = jr.PRNGKey(key)\n",
    "\n",
    "    # Generate correlated raw data\n",
    "    theta_train = your_prior_sampler(n_samples, key)\n",
    "    key, *sim_keys = jr.split(key, n_samples + 1)\n",
    "    x_train = jnp.array([your_simulator(theta_train[i], sim_keys[i])\n",
    "                         for i in range(n_samples)])\n",
    "\n",
    "    # Check data correlation\n",
    "    print(\"\\nChecking data correlation...\")\n",
    "    corr_matrix = jnp.corrcoef(x_train.T)\n",
    "    off_diagonal = corr_matrix - jnp.diag(jnp.diag(corr_matrix))\n",
    "    max_correlation = jnp.max(jnp.abs(off_diagonal))\n",
    "    print(f\"Maximum off-diagonal correlation coefficient: {max_correlation:.4f}\")\n",
    "    if max_correlation > 0.3:\n",
    "        print(\"Data confirmed to be correlated\")\n",
    "    else:\n",
    "        print(\"Warning: Data correlation is weak\")\n",
    "\n",
    "    print(\"\\nComputing training features (optimized: compute pairwise first, then sum)...\")\n",
    "\n",
    "    # Core optimization: compute pairwise scores grouped by distance first\n",
    "    print(\"   Step 1: Compute grouped pairwise features...\")\n",
    "    x_train_pairwise_grouped, distance_groups = compute_distance_grouped_pairwise_scores_estimated_theta_numba(x_train)\n",
    "\n",
    "    # Then simply sum to get data score (avoid recomputing 2016 pairs!)\n",
    "    print(\"   Step 2: Sum pairwise features to get data score...\")\n",
    "    x_train_data_scores = jnp.sum(x_train_pairwise_grouped, axis=1, keepdims=True)  # (n_samples, 1)\n",
    "\n",
    "    print(f\"Computation completed:\")\n",
    "    print(f\"   - Pairwise scores for 2016 pairs computed only once\")\n",
    "    print(f\"   - Data Score = sum across distance groups\")\n",
    "    print(f\"   - Computation time significantly reduced\")\n",
    "\n",
    "    # Step 3. Create combined features - simply concatenate\n",
    "    print(\"   Step 3: Create combined features...\")\n",
    "\n",
    "    # Normalize pairwise features\n",
    "    normalized_pairwise = []\n",
    "    for i in range(len(x_train_pairwise_grouped)):\n",
    "        sample_vec = x_train_pairwise_grouped[i]  # vector of distance groups\n",
    "        normalized_vec = (sample_vec - jnp.mean(sample_vec)) / (jnp.std(sample_vec) + 1e-8)\n",
    "        normalized_pairwise.append(normalized_vec)\n",
    "    x_train_pairwise_grouped = jnp.array(normalized_pairwise)\n",
    "\n",
    "    # Concatenate data_score + normalized_pairwise\n",
    "    x_train_combined_grouped = jnp.concatenate([x_train_data_scores, x_train_pairwise_grouped], axis=1)\n",
    "\n",
    "    # Normalize combined features\n",
    "    normalized_combined = []\n",
    "    for i in range(len(x_train_combined_grouped)):\n",
    "        sample_vec = x_train_combined_grouped[i]  # (1 + number of distance groups)-dim vector\n",
    "        normalized_vec = (sample_vec - jnp.mean(sample_vec)) / (jnp.std(sample_vec) + 1e-8)\n",
    "        normalized_combined.append(normalized_vec)\n",
    "    x_train_combined_grouped = jnp.array(normalized_combined)\n",
    "\n",
    "    print(f\"\\n64-point optimized training data preparation completed:\")\n",
    "    print(f\"   - Parameter shape: {theta_train.shape}\")\n",
    "    print(f\"   - Raw data shape: {x_train.shape}\")\n",
    "    print(f\"   - Data Score shape: {x_train_data_scores.shape} (obtained via pairwise summation)\")\n",
    "    print(f\"   - Pairwise Grouped shape: {x_train_pairwise_grouped.shape}\")\n",
    "    print(f\"   - Combined Grouped shape: {x_train_combined_grouped.shape}\")\n",
    "    print(f\"   - Number of distance groups: {len(distance_groups)}\")\n",
    "\n",
    "    print(f\"\\nOptimization effect:\")\n",
    "    print(f\"   - Original version: required 2016*2 = 4032 pairwise score computations\")\n",
    "    print(f\"   - Optimized version: only 2016*1 = 2016 computations\")\n",
    "    print(f\"   - Speed improvement: ~2x\")\n",
    "\n",
    "    return (theta_train, x_train, x_train_data_scores,\n",
    "            x_train_pairwise_grouped, x_train_combined_grouped, distance_groups)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaeda24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272176,
     "status": "ok",
     "timestamp": 1757542022057,
     "user": {
      "displayName": "Shanzi Bao",
      "userId": "16567399982080387951"
     },
     "user_tz": -480
    },
    "id": "daaeda24",
    "outputId": "60280fec-d779-4262-b326-a5fa019a6dd3"
   },
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    \"\"\"Generate training data using existing functions\"\"\"\n",
    "    print(f\"Generating {CONFIG['n_training_samples']} training samples...\")\n",
    "\n",
    "    key = jr.PRNGKey(CONFIG['RANDOM_SEED'])\n",
    "    theta_train = your_prior_sampler(CONFIG['n_training_samples'], key)\n",
    "\n",
    "    key, *sim_keys = jr.split(key, CONFIG['n_training_samples'] + 1)\n",
    "    x_train = jnp.array([your_simulator(theta_train[i], sim_keys[i])\n",
    "                        for i in range(CONFIG['n_training_samples'])])\n",
    "\n",
    "    print(f\"Training data shape: theta={theta_train.shape}, x={x_train.shape}\")\n",
    "    return theta_train, x_train\n",
    "\n",
    "# Generate once, use for all methods\n",
    "theta_train, x_train = generate_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbec04f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384027,
     "status": "ok",
     "timestamp": 1757542406098,
     "user": {
      "displayName": "Shanzi Bao",
      "userId": "16567399982080387951"
     },
     "user_tz": -480
    },
    "id": "cdbec04f",
    "outputId": "2a8bd316-ef52-4b10-d1e1-66d864a3cd43"
   },
   "outputs": [],
   "source": [
    "def prepare_all_features(theta_train, x_train):\n",
    "    \"\"\"Prepare all feature types using existing optimized functions\"\"\"\n",
    "    print(\"Preparing features for all methods...\")\n",
    "\n",
    "    # Use your existing optimized feature computation\n",
    "    (theta_train, x_train, x_train_data_scores,\n",
    "     x_train_pairwise_grouped, x_train_combined_grouped,\n",
    "     distance_groups) = prepare_all_data_correlated_simplified_optimized(\n",
    "        n_samples=CONFIG['n_training_samples'],\n",
    "        key=CONFIG['RANDOM_SEED']\n",
    "    )\n",
    "\n",
    "    features = {\n",
    "        'raw': x_train,\n",
    "        'data_score': x_train_data_scores,\n",
    "        'pairwise_grouped': x_train_pairwise_grouped,\n",
    "        'combined_grouped': x_train_combined_grouped\n",
    "    }\n",
    "\n",
    "    print(\"Feature dimensions:\")\n",
    "    for name, feat in features.items():\n",
    "        print(f\"  {name}: {feat.shape}\")\n",
    "\n",
    "    return features\n",
    "\n",
    "features = prepare_all_features(theta_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c15973",
   "metadata": {
    "id": "c8c15973"
   },
   "outputs": [],
   "source": [
    "def train_single_method(method_name, theta_train, x_input, train_key, dim_data=64):  # 🔥 changed from 16 to 64\n",
    "    \"\"\"Train a single method\"\"\"\n",
    "    print(f\"\\n Training {method_name} method...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    key = jr.PRNGKey(train_key)\n",
    "    config = create_range_parameter_config()\n",
    "\n",
    "    # Adjust data dimension\n",
    "    config.algorithm.dim_data = dim_data\n",
    "\n",
    "    sde = get_sde(config)\n",
    "    model = NCMLP(key, config)\n",
    "\n",
    "    trained_model, ds_mean, ds_std = train_score_network(\n",
    "        config, model, sde, theta_train, x_input, key\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"{method_name} training completed, time taken: {training_time:.2f} seconds\")\n",
    "\n",
    "    return {\n",
    "        'trained_model': trained_model,\n",
    "        'ds_mean': ds_mean,\n",
    "        'ds_std': ds_std,\n",
    "        'sde': sde,\n",
    "        'config': config,\n",
    "        'training_time': training_time,\n",
    "        'method': method_name\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1dea83",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a1dea83"
   },
   "outputs": [],
   "source": [
    "def train_all_cnf_methods(theta_train, features):\n",
    "    \"\"\"Train CNF models for all feature types\"\"\"\n",
    "    print(\"Training CNF models...\")\n",
    "\n",
    "    trained_models = {}\n",
    "\n",
    "    for method_name in ['raw', 'data_score', 'pairwise_grouped', 'combined_grouped']:\n",
    "        if method_name not in CONFIG['methods_to_test']:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTraining {method_name} model...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use your existing training function\n",
    "        x_input = features[method_name]\n",
    "        dim_data = x_input.shape[1]\n",
    "\n",
    "        trained_models[method_name] = train_single_method(\n",
    "            method_name, theta_train, x_input,\n",
    "            train_key=CONFIG['RANDOM_SEED'],\n",
    "            dim_data=dim_data\n",
    "        )\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.1f}s\")\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "trained_models = train_all_cnf_methods(theta_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfed5e1",
   "metadata": {
    "id": "3bfed5e1",
    "outputId": "e1549851-b7f7-44ef-b3ec-6f90b50e3a5f"
   },
   "outputs": [],
   "source": [
    "def create_cnf_samplers(trained_models):\n",
    "    \"\"\"Create CNF samplers from trained models\"\"\"\n",
    "    print(\"Creating CNF samplers...\")\n",
    "\n",
    "    cnf_samplers = {}\n",
    "\n",
    "    for method_name, model_info in trained_models.items():\n",
    "        cnf = CNF(\n",
    "            score_network=model_info['trained_model'],\n",
    "            sde=model_info['sde'],\n",
    "            ds_means=model_info['ds_mean'],\n",
    "            ds_stds=model_info['ds_std']\n",
    "        )\n",
    "\n",
    "        cnf_samplers[method_name] = {\n",
    "            'cnf': cnf,\n",
    "            'config': model_info['config'],\n",
    "            'training_time': model_info['training_time']\n",
    "        }\n",
    "\n",
    "    print(f\"Created {len(cnf_samplers)} CNF samplers\")\n",
    "    return cnf_samplers\n",
    "\n",
    "cnf_samplers = create_cnf_samplers(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67166b7b",
   "metadata": {
    "id": "67166b7b"
   },
   "outputs": [],
   "source": [
    "def run_single_inference(method_name, x_obs, cnf_samplers=None):\n",
    "    \"\"\"Run inference for a single method\"\"\"\n",
    "\n",
    "    if method_name in ['abc', 'abc_adjusted']:\n",
    "        # Use your existing ABC functions\n",
    "        use_adjustment = (method_name == 'abc_adjusted')\n",
    "        samples = run_abc_inference_with_adjustment(\n",
    "            x_obs,\n",
    "            n_samples=CONFIG['n_posterior_samples'],\n",
    "            apply_adjustment=use_adjustment,\n",
    "            verbose=False\n",
    "        )\n",
    "        return samples, 0.0  # ABC has no training time\n",
    "\n",
    "    else:\n",
    "        # CNF methods\n",
    "        cnf_info = cnf_samplers[method_name]\n",
    "\n",
    "        # Prepare features using your existing function\n",
    "        x_input = compute_test_input_fixed_optimized_numba(x_obs, method_name)\n",
    "\n",
    "        # Sample from posterior\n",
    "        key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "        samples = cnf_info['cnf'].batch_sample_fn(\n",
    "            CONFIG['n_posterior_samples'], x_input, key\n",
    "        )\n",
    "\n",
    "        return samples, cnf_info['training_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b0d3c",
   "metadata": {
    "id": "dc9b0d3c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import jit, prange\n",
    "from collections import defaultdict\n",
    "\n",
    "@jit(nopython=True)\n",
    "def create_2d_grid_numba():\n",
    "    \"\"\"创建8x8网格点 - numba优化版本\"\"\"\n",
    "    coords = np.linspace(0, 1, 8)\n",
    "    locations = np.zeros((64, 2))\n",
    "    idx = 0\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            locations[idx, 0] = coords[i]\n",
    "            locations[idx, 1] = coords[j]\n",
    "            idx += 1\n",
    "    return locations\n",
    "\n",
    "@jit(nopython=True)\n",
    "def estimate_range_by_correlation_numba(x_obs):\n",
    "    \"\"\"基于8x8相邻点相关性估计range parameter - numba优化版本\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # 计算相邻点的经验相关性代理\n",
    "    x_mean = np.mean(x_obs)\n",
    "    x_std = np.std(x_obs)\n",
    "    if x_std < 1e-8:\n",
    "        x_std = 1e-8\n",
    "    x_normalized = (x_obs - x_mean) / x_std\n",
    "\n",
    "    # 定义相邻点对 - 使用静态数组\n",
    "    neighbor_pairs = np.array([\n",
    "        # 水平相邻 (部分)\n",
    "        [0,1], [1,2], [2,3], [8,9], [9,10], [16,17], [24,25],\n",
    "        # 垂直相邻 (部分)\n",
    "        [0,8], [1,9], [8,16], [16,24], [24,32], [32,40], [40,48], [48,56],\n",
    "        # 对角相邻 (部分)\n",
    "        [0,9], [1,10], [8,17], [16,25]\n",
    "    ])\n",
    "\n",
    "    correlations = []\n",
    "    distances = []\n",
    "\n",
    "    for k in range(neighbor_pairs.shape[0]):\n",
    "        i, j = neighbor_pairs[k, 0], neighbor_pairs[k, 1]\n",
    "        if i < 64 and j < 64:\n",
    "            corr = x_normalized[i] * x_normalized[j]\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            dist = np.sqrt(dist_sq)\n",
    "            if corr > 0.01:\n",
    "                correlations.append(corr)\n",
    "                distances.append(dist)\n",
    "\n",
    "    if len(correlations) > 0:\n",
    "        avg_corr = np.mean(np.array(correlations))\n",
    "        avg_dist = np.mean(np.array(distances))\n",
    "\n",
    "        # 反推range: corr = exp(-dist/range)\n",
    "        if avg_corr > 0:\n",
    "            range_est = -avg_dist / np.log(avg_corr)\n",
    "            # 限制在合理范围内\n",
    "            return min(max(range_est, 5.0), 95.0)\n",
    "\n",
    "    # 如果相关性都太小，使用先验均值\n",
    "    return 50.0\n",
    "\n",
    "@jit(nopython=True)\n",
    "def compute_distance_groups_numba():\n",
    "    \"\"\"预计算所有距离分组信息 - numba优化版本\"\"\"\n",
    "    locations = create_2d_grid_numba()\n",
    "\n",
    "    # 计算所有距离并找到唯一值\n",
    "    unique_distances = []\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    for i in range(64):\n",
    "        for j in range(i+1, 64):\n",
    "            dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "            dist = np.sqrt(dist_sq)\n",
    "\n",
    "            # 检查是否为新的唯一距离\n",
    "            is_unique = True\n",
    "            for existing_dist in unique_distances:\n",
    "                if abs(dist - existing_dist) < tolerance:\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                unique_distances.append(dist)\n",
    "\n",
    "    # 排序距离\n",
    "    unique_distances.sort()\n",
    "    return np.array(unique_distances), locations\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_pairwise_grouped_scores_numba(x_obs, theta_estimated, unique_distances, locations):\n",
    "    \"\"\"计算距离分组的pairwise scores - numba优化版本\"\"\"\n",
    "    group_scores = np.zeros(len(unique_distances))\n",
    "    sigma = 5.0\n",
    "    tolerance = 1e-3\n",
    "\n",
    "    # 并行计算每个距离组的分数\n",
    "    for dist_idx in prange(len(unique_distances)):\n",
    "        target_dist = unique_distances[dist_idx]\n",
    "        group_score_sum = 0.0\n",
    "\n",
    "        # 找到所有匹配这个距离的点对\n",
    "        for i in range(64):\n",
    "            for j in range(i+1, 64):\n",
    "                actual_dist_sq = (locations[i, 0] - locations[j, 0])**2 + (locations[i, 1] - locations[j, 1])**2\n",
    "                actual_dist = np.sqrt(actual_dist_sq)\n",
    "\n",
    "                if abs(actual_dist - target_dist) < tolerance:\n",
    "                    # 计算pairwise score\n",
    "                    v_ij = x_obs[i] - x_obs[j]\n",
    "                    exp_term = np.exp(-target_dist / theta_estimated)\n",
    "                    gamma_ij = sigma**2 * (1 - exp_term)\n",
    "                    gamma_ij = max(gamma_ij, 1e-8)\n",
    "                    dgamma_dtheta = sigma**2 * exp_term * target_dist / (theta_estimated**2)\n",
    "                    score_ij = dgamma_dtheta / (2 * gamma_ij) * (v_ij**2 / (2 * gamma_ij) - 1)\n",
    "\n",
    "                    group_score_sum += score_ij\n",
    "\n",
    "        group_scores[dist_idx] = group_score_sum\n",
    "\n",
    "    return group_scores\n",
    "\n",
    "@jit(nopython=True)\n",
    "def normalize_array_numba(arr):\n",
    "    \"\"\"数组标准化 - numba优化版本\"\"\"\n",
    "    mean_val = np.mean(arr)\n",
    "    std_val = np.std(arr)\n",
    "    if std_val < 1e-8:\n",
    "        std_val = 1e-8\n",
    "    return (arr - mean_val) / std_val\n",
    "\n",
    "# 预计算静态数据（只需要计算一次）\n",
    "_unique_distances, _locations = compute_distance_groups_numba()\n",
    "\n",
    "def compute_test_input_fixed_optimized_numba(x_obs, method):\n",
    "    \"\"\"优化版本的测试输入计算函数 - numba优化版本\"\"\"\n",
    "    x_obs_array = np.array(x_obs)\n",
    "\n",
    "    if method == 'raw':\n",
    "        return x_obs_array  # 64维原始数据\n",
    "\n",
    "    # 估计theta (8x8版本)\n",
    "    theta_estimated = estimate_range_by_correlation_numba(x_obs_array)\n",
    "\n",
    "    # 只计算一次pairwise grouped scores (避免重复计算2016个点对!)\n",
    "    pairwise_scores_raw = compute_pairwise_grouped_scores_numba(\n",
    "        x_obs_array, theta_estimated, _unique_distances, _locations\n",
    "    )\n",
    "\n",
    "    if method == 'data_score':\n",
    "        # 简单求和得到data score\n",
    "        total_score = np.sum(pairwise_scores_raw)\n",
    "        return np.array([total_score])\n",
    "\n",
    "    elif method == 'pairwise_grouped':\n",
    "        # 直接返回归一化的pairwise scores\n",
    "        normalized_result = normalize_array_numba(pairwise_scores_raw)\n",
    "        return normalized_result\n",
    "\n",
    "    elif method == 'combined_grouped':\n",
    "        # 组合：data_score + normalized_pairwise\n",
    "        data_score_part = np.array([np.sum(pairwise_scores_raw)])  # 1维\n",
    "\n",
    "        pairwise_part = normalize_array_numba(pairwise_scores_raw)\n",
    "\n",
    "        # 合并并归一化\n",
    "        combined_result = np.concatenate((data_score_part, pairwise_part))\n",
    "        combined_result = normalize_array_numba(combined_result)\n",
    "        return combined_result\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded254db",
   "metadata": {
    "id": "ded254db"
   },
   "outputs": [],
   "source": [
    "def compute_test_input_fixed_optimized_numba(x_obs, method):\n",
    "    \"\"\"优化版本的测试输入计算函数 - numba优化版本\"\"\"\n",
    "    x_obs_array = np.array(x_obs)\n",
    "\n",
    "    if method == 'raw':\n",
    "        return x_obs_array  # 64维原始数据\n",
    "\n",
    "    # 估计theta (8x8版本)\n",
    "    theta_estimated = estimate_range_by_correlation_numba(x_obs_array)\n",
    "\n",
    "    # 只计算一次pairwise grouped scores (避免重复计算2016个点对!)\n",
    "    pairwise_scores_raw = compute_pairwise_grouped_scores_numba(\n",
    "        x_obs_array, theta_estimated, _unique_distances, _locations\n",
    "    )\n",
    "\n",
    "    if method == 'data_score':\n",
    "        # 简单求和得到data score\n",
    "        total_score = np.sum(pairwise_scores_raw)\n",
    "        return np.array([total_score])\n",
    "\n",
    "    elif method == 'pairwise_grouped':\n",
    "        # 直接返回归一化的pairwise scores\n",
    "        normalized_result = normalize_array_numba(pairwise_scores_raw)\n",
    "        return normalized_result\n",
    "\n",
    "    elif method == 'combined_grouped':\n",
    "        # 组合：data_score + normalized_pairwise\n",
    "        data_score_part = np.array([np.sum(pairwise_scores_raw)])  # 1维\n",
    "\n",
    "        pairwise_part = normalize_array_numba(pairwise_scores_raw)\n",
    "\n",
    "        # 合并并归一化\n",
    "        combined_result = np.concatenate((data_score_part, pairwise_part))\n",
    "        combined_result = normalize_array_numba(combined_result)\n",
    "        return combined_result\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c286b",
   "metadata": {
    "id": "3a8c286b",
    "outputId": "c3d9d251-a631-4433-ac9d-b7b271f5fe1b"
   },
   "outputs": [],
   "source": [
    "# 在原有的4种方法基础上添加ABC作为第5种方法\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from pyabc import ABCSMC, DistributionBase, Parameter\n",
    "from scipy.stats import uniform\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 首先运行ABC\n",
    "print(\"🔬 运行ABC推断...\")\n",
    "\n",
    "# 使用你的完整ABC实现\n",
    "def gaussian_field_model(params):\n",
    "    \"\"\"Gaussian Field模拟器 - 返回原始数据\"\"\"\n",
    "    # 添加调试计数器\n",
    "    if not hasattr(gaussian_field_model, '_call_count'):\n",
    "        gaussian_field_model._call_count = 0\n",
    "        gaussian_field_model._success_count = 0\n",
    "    gaussian_field_model._call_count += 1\n",
    "\n",
    "    # 转换参数\n",
    "    theta = np.array([params['range_param']])\n",
    "\n",
    "    # 验证参数\n",
    "    if theta[0] <= 0 or theta[0] >= 100:\n",
    "        if gaussian_field_model._call_count <= 5:\n",
    "            print(f\"模拟器 #{gaussian_field_model._call_count}: 参数验证失败\")\n",
    "        # 返回简单的替代数据而不是NaN\n",
    "        return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "    try:\n",
    "        # 生成模拟数据\n",
    "        key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "        sim_data = your_simulator(jnp.array(theta), key)  # (64,)\n",
    "\n",
    "        if sim_data is None or np.any(np.isnan(sim_data)) or np.any(np.isinf(sim_data)):\n",
    "            if gaussian_field_model._call_count <= 5:\n",
    "                print(f\"模拟器 #{gaussian_field_model._call_count}: 模拟数据无效\")\n",
    "            # 返回简单的替代数据而不是NaN\n",
    "            return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "        # 成功生成数据\n",
    "        gaussian_field_model._success_count += 1\n",
    "        if gaussian_field_model._call_count <= 5:\n",
    "            print(f\"模拟器 #{gaussian_field_model._call_count}: 成功生成数据，形状={sim_data.shape}\")\n",
    "\n",
    "        # 确保返回正确的数据类型，并进行数值检查\n",
    "        sim_array = np.array(sim_data, dtype=np.float64)\n",
    "\n",
    "        # 替换任何剩余的NaN或inf值\n",
    "        if np.any(np.isnan(sim_array)) or np.any(np.isinf(sim_array)):\n",
    "            print(f\"发现无效值，使用替代数据\")\n",
    "            return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "        return {\"data\": sim_array}\n",
    "\n",
    "    except Exception as e:\n",
    "        if gaussian_field_model._call_count <= 5:\n",
    "            print(f\"模拟器 #{gaussian_field_model._call_count}: 异常 - {str(e)}\")\n",
    "        # 返回简单的替代数据而不是NaN\n",
    "        return {\"data\": np.full(64, 0.0, dtype=np.float64)}\n",
    "\n",
    "\n",
    "class GaussianFieldPrior(DistributionBase):\n",
    "    \"\"\"Gaussian Field先验分布\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # 简单的uniform先验 range ~ uniform(0, 100)\n",
    "        self.range_min = 0.0\n",
    "        self.range_max = 100.0\n",
    "        self.uniform_dist = uniform(loc=self.range_min, scale=self.range_max - self.range_min)\n",
    "\n",
    "        self.param_names = ['range_param']\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"采样 - 简单的均匀分布\"\"\"\n",
    "        max_attempts = 100\n",
    "        for attempt in range(max_attempts):\n",
    "            # 从均匀分布采样\n",
    "            raw_sample = self.uniform_dist.rvs()\n",
    "\n",
    "            # 验证参数\n",
    "            if self._validate_sample(raw_sample):\n",
    "                sample_dict = {'range_param': float(raw_sample)}\n",
    "                return Parameter(sample_dict)\n",
    "\n",
    "        # 如果所有尝试都失败，返回中间值\n",
    "        print(f\"先验采样失败，使用中间值\")\n",
    "        sample_dict = {'range_param': 50.0}\n",
    "        return Parameter(sample_dict)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        \"\"\"计算密度 - 均匀分布\"\"\"\n",
    "        if not isinstance(x, dict):\n",
    "            x = x.to_dict() if hasattr(x, 'to_dict') else dict(x)\n",
    "\n",
    "        # 转换为数组\n",
    "        theta = x['range_param']\n",
    "\n",
    "        # 验证参数约束\n",
    "        if not self._validate_sample(theta):\n",
    "            return 1e-10\n",
    "\n",
    "        # 均匀分布密度\n",
    "        try:\n",
    "            density = self.uniform_dist.pdf(theta)\n",
    "            density_val = max(float(density), 1e-10)\n",
    "\n",
    "            # 检查密度是否有效\n",
    "            if np.isnan(density_val) or np.isinf(density_val):\n",
    "                return 1e-10\n",
    "\n",
    "            return density_val\n",
    "        except Exception:\n",
    "            return 1e-10\n",
    "\n",
    "    def _validate_sample(self, theta):\n",
    "        \"\"\"参数验证\"\"\"\n",
    "        # range参数范围\n",
    "        if theta <= 0.0 or theta >= 100.0:\n",
    "            return False\n",
    "\n",
    "        # 检查数值稳定性\n",
    "        if np.isnan(theta) or np.isinf(theta):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "def gaussian_field_distance(x, y):\n",
    "    \"\"\"距离函数 - 使用combined_grouped作为汇总统计\"\"\"\n",
    "    data_x = x[\"data\"]  # (64,)\n",
    "    data_y = y[\"data\"]  # (64,)\n",
    "\n",
    "    # 检查数据有效性\n",
    "    if np.any(np.isnan(data_x)) or np.any(np.isnan(data_y)):\n",
    "        return 1e8\n",
    "\n",
    "    if np.any(np.isinf(data_x)) or np.any(np.isinf(data_y)):\n",
    "        return 1e8\n",
    "\n",
    "    try:\n",
    "        # 使用combined_grouped作为summary statistics\n",
    "        # 确保数组类型正确\n",
    "        data_x = np.asarray(data_x, dtype=np.float64)\n",
    "        data_y = np.asarray(data_y, dtype=np.float64)\n",
    "        summary_x = compute_test_input_fixed_optimized_numba(data_x, 'combined_grouped')\n",
    "        summary_y = compute_test_input_fixed_optimized_numba(data_y, 'combined_grouped')\n",
    "\n",
    "        # 检查汇总统计是否有效\n",
    "        if np.any(np.isnan(summary_x)) or np.any(np.isnan(summary_y)):\n",
    "            return 1e8\n",
    "\n",
    "        if np.any(np.isinf(summary_x)) or np.any(np.isinf(summary_y)):\n",
    "            return 1e8\n",
    "\n",
    "        # 欧式距离比较汇总统计\n",
    "        distance = np.sqrt(np.sum((summary_x - summary_y)**2))\n",
    "\n",
    "        # 检查距离是否有效\n",
    "        if np.isnan(distance) or np.isinf(distance) or distance < 0:\n",
    "            return 1e8\n",
    "\n",
    "        return float(distance)\n",
    "\n",
    "    except Exception as e:\n",
    "        return 1e10  # 明确表示这个样本不好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52168c1f",
   "metadata": {
    "id": "52168c1f"
   },
   "outputs": [],
   "source": [
    "# Complete ABC Regression Adjustment Integration Code\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from pyabc import ABCSMC, DistributionBase, Parameter\n",
    "from scipy.stats import uniform, gaussian_kde\n",
    "import tempfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from collections import defaultdict\n",
    "\n",
    "# ========== Helper Functions ==========\n",
    "def _coerce_to_array(item, target_shape):\n",
    "    \"\"\"Convert various formats to numpy array\"\"\"\n",
    "    try:\n",
    "        if hasattr(item, 'numpy'):  # JAX/PyTorch\n",
    "            arr = np.array(item.numpy(), dtype=float)\n",
    "        elif hasattr(item, 'values'):  # pandas\n",
    "            arr = np.array(item.values, dtype=float)\n",
    "        else:\n",
    "            arr = np.array(item, dtype=float)\n",
    "\n",
    "        # Try to reshape to target shape\n",
    "        if arr.shape != target_shape:\n",
    "            if arr.size == np.prod(target_shape):\n",
    "                arr = arr.reshape(target_shape)\n",
    "            else:\n",
    "                return None\n",
    "        return arr\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def _epanechnikov_weights(z, bandwidth):\n",
    "    \"\"\"Epanechnikov kernel weights\"\"\"\n",
    "    abs_z = np.abs(z / bandwidth)\n",
    "    w = np.where(abs_z <= 1, 0.75 * (1 - abs_z**2), 0)\n",
    "    return w\n",
    "\n",
    "def _ess(weights):\n",
    "    \"\"\"Effective sample size\"\"\"\n",
    "    w = np.asarray(weights)\n",
    "    w = w[np.isfinite(w) & (w > 0)]\n",
    "    if len(w) == 0:\n",
    "        return 0\n",
    "    return np.sum(w)**2 / np.sum(w**2)\n",
    "\n",
    "def _wls(X, y, weights, ridge=1e-8):\n",
    "    \"\"\"Weighted least squares\"\"\"\n",
    "    W = np.diag(weights)\n",
    "    XtWX = X.T @ W @ X\n",
    "    XtWX += ridge * np.eye(XtWX.shape[0])\n",
    "    XtWy = X.T @ W @ y\n",
    "\n",
    "    try:\n",
    "        beta = np.linalg.solve(XtWX, XtWy)\n",
    "        yhat = X @ beta\n",
    "\n",
    "        # R-squared calculation\n",
    "        y_mean = np.average(y, weights=weights)\n",
    "        ss_tot = np.sum(weights * (y - y_mean)**2)\n",
    "        ss_res = np.sum(weights * (y - yhat)**2)\n",
    "        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "        return beta, yhat, r2\n",
    "    except:\n",
    "        return np.zeros(X.shape[1]), np.zeros(len(y)), 0\n",
    "\n",
    "# ========== Core Regression Adjustment Function ==========\n",
    "def abc_regression_adjustment(\n",
    "    theta_samples,\n",
    "    summary_stats_list,\n",
    "    observed_summary_matrix,\n",
    "    bandwidth_quantile=0.8,\n",
    "    min_effective_n=30,\n",
    "    ridge=0,\n",
    "    feature_clip=6.0,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    ABC regression adjustment - adapted for your code structure\n",
    "    \"\"\"\n",
    "    theta_samples = np.asarray(theta_samples, dtype=float)\n",
    "    n_samples, n_params = theta_samples.shape\n",
    "\n",
    "    obs = np.asarray(observed_summary_matrix, dtype=float)\n",
    "    obs_shape = obs.shape\n",
    "    obs_flat = obs.reshape(-1)\n",
    "    obs_finite_flat = np.isfinite(obs_flat)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ABC regression adjustment: {n_samples} samples, feature dimension {obs_shape}\")\n",
    "\n",
    "    # ========== Calculate distances ==========\n",
    "    dists = np.full(n_samples, np.inf, dtype=float)\n",
    "    coerced_list = [None] * n_samples\n",
    "    converted, failed = 0, 0\n",
    "\n",
    "    for k in range(n_samples):\n",
    "        if k >= len(summary_stats_list):\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        sim = _coerce_to_array(summary_stats_list[k], obs_shape)\n",
    "        if sim is None:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        coerced_list[k] = sim\n",
    "        sim_flat = sim.reshape(-1)\n",
    "        both = obs_finite_flat & np.isfinite(sim_flat)\n",
    "        if not np.any(both):\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        diff = (sim_flat - obs_flat)[both]\n",
    "        dists[k] = np.sqrt(np.sum(diff * diff))\n",
    "        converted += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Distance calculation: succeeded={converted}, failed={failed}\")\n",
    "\n",
    "    valid = np.isfinite(dists)\n",
    "    if not np.any(valid):\n",
    "        if verbose:\n",
    "            print(\"Error: no valid distance calculations\")\n",
    "        return theta_samples.copy()\n",
    "\n",
    "    # ========== Standardize distances and bandwidth ==========\n",
    "    z = dists[valid] - dists[valid].mean()\n",
    "    std = dists[valid].std()\n",
    "    z = z / std if std > 0 else np.zeros_like(z)\n",
    "\n",
    "    bandwidth = np.quantile(np.abs(z), bandwidth_quantile)\n",
    "    if not np.isfinite(bandwidth) or bandwidth <= 0:\n",
    "        bandwidth = 1.0\n",
    "\n",
    "    z_all = np.full_like(dists, np.nan, dtype=float)\n",
    "    z_all[valid] = (dists[valid] - dists[valid].mean()) / (std if std > 0 else 1.0)\n",
    "\n",
    "    # ========== Kernel weights ==========\n",
    "    w = _epanechnikov_weights(z_all, bandwidth)\n",
    "    keep_idx = np.where(w > 0)[0]\n",
    "    ess = _ess(w)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Bandwidth: {bandwidth:.6f}, effective samples: {keep_idx.size}, ESS: {ess:.1f}\")\n",
    "\n",
    "    if keep_idx.size < 5:\n",
    "        if verbose:\n",
    "            print(\"Too few effective samples, returning original samples\")\n",
    "        return theta_samples.copy()\n",
    "\n",
    "    # ========== Construct design matrix ==========\n",
    "    mask = np.isfinite(obs_flat)\n",
    "    p = int(mask.sum())\n",
    "\n",
    "    X_rows = []\n",
    "    kept_ok = []\n",
    "    for k in keep_idx:\n",
    "        sim = coerced_list[k]\n",
    "        if sim is None:\n",
    "            continue\n",
    "        sim2_flat = sim.reshape(-1).copy()\n",
    "        bad = ~np.isfinite(sim2_flat)\n",
    "        if np.any(bad):\n",
    "            sim2_flat[bad] = obs_flat[bad]\n",
    "        diff_vec = (sim2_flat - obs_flat)[mask]\n",
    "        X_rows.append(diff_vec)\n",
    "        kept_ok.append(k)\n",
    "\n",
    "    if len(X_rows) < 5:\n",
    "        if verbose:\n",
    "            print(\"Insufficient samples after design matrix construction\")\n",
    "        return theta_samples.copy()\n",
    "\n",
    "    kept_ok = np.asarray(kept_ok, dtype=int)\n",
    "    X = np.asarray(X_rows)\n",
    "\n",
    "    # Column standardization\n",
    "    col_mean = X.mean(axis=0)\n",
    "    col_std = X.std(axis=0, ddof=0)\n",
    "    col_std[col_std == 0] = 1.0\n",
    "    Xs = (X - col_mean) / col_std\n",
    "\n",
    "    if feature_clip is not None and feature_clip > 0:\n",
    "        np.clip(Xs, -feature_clip, feature_clip, out=Xs)\n",
    "\n",
    "    # Add intercept\n",
    "    Xs = np.column_stack([np.ones(Xs.shape[0]), Xs])\n",
    "\n",
    "    # ========== Parameter-wise regression adjustment ==========\n",
    "    theta_adj = theta_samples.copy()\n",
    "\n",
    "    for pidx in range(n_params):\n",
    "        if verbose:\n",
    "            print(f\"Adjusting parameter {pidx+1}/{n_params}\")\n",
    "\n",
    "        y = theta_samples[kept_ok, pidx]\n",
    "        beta, yhat, r2 = _wls(Xs, y, w[kept_ok], ridge=ridge)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  R²: {r2:.4f}\")\n",
    "\n",
    "        # Only adjust samples with kernel weights\n",
    "        for k in kept_ok:\n",
    "            sim = coerced_list[k]\n",
    "            if sim is None:\n",
    "                continue\n",
    "            sim2_flat = sim.reshape(-1).copy()\n",
    "            bad = ~np.isfinite(sim2_flat)\n",
    "            if np.any(bad):\n",
    "                sim2_flat[bad] = obs_flat[bad]\n",
    "            diff_vec = (sim2_flat - obs_flat)[mask]\n",
    "            xk = (diff_vec - col_mean) / col_std\n",
    "            if feature_clip is not None and feature_clip > 0:\n",
    "                np.clip(xk, -feature_clip, feature_clip, out=xk)\n",
    "            xk = np.concatenate([[1.0], xk])\n",
    "            yk_pred = xk @ beta\n",
    "            adjustment = yk_pred - beta[0]\n",
    "            theta_adj[k, pidx] = theta_samples[k, pidx] - adjustment\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Regression adjustment complete!\")\n",
    "\n",
    "    return theta_adj\n",
    "\n",
    "# ========== Enhanced ABC Implementation ==========\n",
    "def run_abc_inference_with_adjustment(x_obs, n_samples, apply_adjustment=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Enhanced ABC inference with regression adjustment\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Starting ABC inference (samples: {n_samples}, regression adjustment: {apply_adjustment})\")\n",
    "\n",
    "    # Reset simulator counters\n",
    "    if hasattr(gaussian_field_model, '_call_count'):\n",
    "        gaussian_field_model._call_count = 0\n",
    "        gaussian_field_model._success_count = 0\n",
    "\n",
    "    # ABC setup\n",
    "    prior = GaussianFieldPrior()\n",
    "\n",
    "    abc = ABCSMC(\n",
    "        models=gaussian_field_model,\n",
    "        parameter_priors=prior,\n",
    "        distance_function=gaussian_field_distance,\n",
    "        population_size= n_samples,\n",
    "        sampler=pyabc.sampler.SingleCoreSampler(),\n",
    "        eps=pyabc.epsilon.QuantileEpsilon(alpha=0.5, quantile_multiplier=0.95)\n",
    "    )\n",
    "\n",
    "    # Prepare observed data\n",
    "    obs = {\"data\": np.array(x_obs, dtype=np.float64)}\n",
    "\n",
    "    # Create database\n",
    "    db_path = os.path.join(tempfile.gettempdir(), f\"abc_adjusted_{int(time.time())}.db\")\n",
    "    abc.new(\"sqlite:///\" + db_path, obs)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Running ABC sampling...\")\n",
    "\n",
    "    try:\n",
    "        # Run ABC\n",
    "        history = abc.run(max_nr_populations=3)\n",
    "\n",
    "        # Extract samples\n",
    "        df, w = history.get_distribution(m=0, t=history.max_t)\n",
    "        theta_samples = df['range_param'].values\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ABC complete: {len(theta_samples)} samples\")\n",
    "\n",
    "        # ========== Apply regression adjustment ==========\n",
    "        if apply_adjustment and len(theta_samples) > 50:\n",
    "            if verbose:\n",
    "                print(\"Starting regression adjustment...\")\n",
    "\n",
    "            # Recompute summary statistics for regression adjustment\n",
    "            summary_stats_list = []\n",
    "            obs_summary = compute_test_input_fixed_optimized_numba(x_obs, 'combined_grouped')\n",
    "\n",
    "            # Recompute summary statistics for each ABC sample\n",
    "            n_for_adjustment = min(len(theta_samples), 500)  # Limit number to avoid slowdown\n",
    "            for i in range(n_for_adjustment):\n",
    "                theta_val = theta_samples[i]\n",
    "                if verbose and i % 100 == 0:\n",
    "                    print(f\"  Recomputing summary statistics {i+1}/{n_for_adjustment}\")\n",
    "\n",
    "                try:\n",
    "                    # Simulate data with this theta value\n",
    "                    key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "                    sim_data = your_simulator(jnp.array([theta_val]), key)\n",
    "                    summary_stats = compute_test_input_fixed_optimized_numba(sim_data, 'combined_grouped')\n",
    "                    summary_stats_list.append(summary_stats)\n",
    "                except Exception as e:\n",
    "                    if verbose and i < 5:\n",
    "                        print(f\"    Sample {i} simulation failed: {e}\")\n",
    "                    # Use observed values when failed\n",
    "                    summary_stats_list.append(obs_summary.copy())\n",
    "\n",
    "            # Only adjust samples for which we recomputed summary statistics\n",
    "            theta_for_adjustment = theta_samples[:n_for_adjustment].reshape(-1, 1)\n",
    "\n",
    "            # Apply regression adjustment\n",
    "            theta_adjusted = abc_regression_adjustment(\n",
    "                theta_samples=theta_for_adjustment,\n",
    "                summary_stats_list=summary_stats_list,\n",
    "                observed_summary_matrix=obs_summary,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            # Combine adjusted and unadjusted samples\n",
    "            final_samples = np.concatenate([\n",
    "                theta_adjusted.flatten(),\n",
    "                theta_samples[n_for_adjustment:]\n",
    "            ])\n",
    "\n",
    "            if verbose:\n",
    "                orig_mean = np.mean(theta_samples)\n",
    "                adj_mean = np.mean(final_samples)\n",
    "                print(f\"Adjustment effect: original mean={orig_mean:.3f}, adjusted mean={adj_mean:.3f}\")\n",
    "        else:\n",
    "            final_samples = theta_samples\n",
    "            if verbose and apply_adjustment:\n",
    "                print(\"Insufficient samples, skipping regression adjustment\")\n",
    "\n",
    "        # Randomly sample specified number if needed\n",
    "        if len(final_samples) > n_samples:\n",
    "            indices = np.random.choice(len(final_samples), n_samples, replace=False)\n",
    "            final_samples = final_samples[indices]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Returning {len(final_samples)} samples\")\n",
    "\n",
    "        # Clean up temporary database\n",
    "        try:\n",
    "            os.remove(db_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return jnp.array(final_samples)\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"ABC failed: {str(e)}\")\n",
    "        # Clean up temporary database\n",
    "        try:\n",
    "            os.remove(db_path)\n",
    "        except:\n",
    "            pass\n",
    "        return jr.uniform(jr.PRNGKey(42), (n_samples,), minval=5.0, maxval=95.0)\n",
    "\n",
    "# ========== Update original ABC function for compatibility ==========\n",
    "def run_abc_inference(x_obs, n_samples=1000, use_adjustment=False):\n",
    "    \"\"\"\n",
    "    Updated ABC inference function - compatible with MSE experiments\n",
    "    \"\"\"\n",
    "    return run_abc_inference_with_adjustment(\n",
    "        x_obs, n_samples=n_samples,\n",
    "        apply_adjustment=use_adjustment,\n",
    "        verbose=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4982e",
   "metadata": {
    "id": "87a4982e"
   },
   "outputs": [],
   "source": [
    "def run_single_inference(method_name, x_obs, cnf_samplers=None):\n",
    "    \"\"\"Run inference for a single method\"\"\"\n",
    "\n",
    "    start_time = time.time()  # 开始计时\n",
    "\n",
    "    if method_name in ['abc', 'abc_adjusted']:\n",
    "        use_adjustment = (method_name == 'abc_adjusted')\n",
    "        samples = run_abc_inference_with_adjustment(\n",
    "            x_obs,\n",
    "            n_samples=CONFIG['n_posterior_samples'],\n",
    "            apply_adjustment=use_adjustment,\n",
    "            verbose=False\n",
    "        )\n",
    "        inference_time = time.time() - start_time  # 计算推理时间\n",
    "        return samples, inference_time, 0.0, CONFIG['n_posterior_samples']\n",
    "        # 返回：样本，推理时间，训练时间(0)，训练样本数(就是后验样本数)\n",
    "\n",
    "    else:\n",
    "        cnf_info = cnf_samplers[method_name]\n",
    "        x_input = compute_test_input_fixed_optimized_numba(x_obs, method_name)\n",
    "\n",
    "        key = jr.PRNGKey(np.random.randint(0, 1000000))\n",
    "        samples = cnf_info['cnf'].batch_sample_fn(\n",
    "            CONFIG['n_posterior_samples'], x_input, key\n",
    "        )\n",
    "\n",
    "        inference_time = time.time() - start_time  # 计算推理时间\n",
    "        return samples, inference_time, cnf_info['training_time'], CONFIG['n_training_samples']\n",
    "        # 返回：样本，推理时间，训练时间，训练样本数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93607017",
   "metadata": {
    "id": "93607017",
    "outputId": "9d6a8cce-3dd6-4d1e-f744-ab5b4d61f836"
   },
   "outputs": [],
   "source": [
    "def run_full_experiment():\n",
    "    \"\"\"Run the complete comparison experiment\"\"\"\n",
    "    print(\"Starting full comparison experiment...\")\n",
    "    print(f\"Testing theta values: {CONFIG['test_theta_values']}\")\n",
    "    print(f\"Methods: {CONFIG['methods_to_test']}\")\n",
    "\n",
    "    results = {method: {\n",
    "    'estimates': [],\n",
    "    'inference_times': [],    # 改名：只存推理时间\n",
    "    'training_time': 0,       # 总训练时间\n",
    "    'n_training_samples': 0   # 新增：训练样本数\n",
    "} for method in CONFIG['methods_to_test']}\n",
    "\n",
    "    total_experiments = len(CONFIG['test_theta_values']) * CONFIG['n_experiments_per_theta']\n",
    "    experiment_count = 0\n",
    "\n",
    "    for theta_idx, true_theta in enumerate(CONFIG['test_theta_values']):\n",
    "        print(f\"\\nTesting theta = {true_theta} ({theta_idx+1}/{len(CONFIG['test_theta_values'])})\")\n",
    "\n",
    "        for exp_idx in range(CONFIG['n_experiments_per_theta']):\n",
    "            experiment_count += 1\n",
    "\n",
    "            if CONFIG['verbose'] and exp_idx % 5 == 0:\n",
    "                print(f\"  Experiment {exp_idx+1}/{CONFIG['n_experiments_per_theta']}\")\n",
    "\n",
    "            # Generate test observation\n",
    "            key = jr.PRNGKey(CONFIG['RANDOM_SEED'] + experiment_count)\n",
    "            theta_array = jnp.array([float(true_theta)])\n",
    "            x_obs = your_simulator(theta_array, key)\n",
    "\n",
    "            # Test each method\n",
    "            for method_name in CONFIG['methods_to_test']:\n",
    "                try:\n",
    "                    # 接收4个返回值\n",
    "                    samples, inference_time, training_time, n_training_samples = run_single_inference(\n",
    "                        method_name, x_obs, cnf_samplers\n",
    "                    )\n",
    "\n",
    "                    # 存储结果\n",
    "                    results[method_name]['inference_times'].append(inference_time)\n",
    "                    results[method_name]['training_time'] = training_time\n",
    "                    results[method_name]['n_training_samples'] = n_training_samples\n",
    "\n",
    "                    # Store results\n",
    "                    posterior_mean = float(jnp.mean(samples))\n",
    "                    results[method_name]['estimates'].append({\n",
    "                        'true_theta': true_theta,\n",
    "                        'estimate': posterior_mean,\n",
    "                        'samples': samples\n",
    "                    })\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    {method_name} failed: {str(e)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "experiment_results = run_full_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b9411",
   "metadata": {
    "id": "258b9411",
    "outputId": "3f9f184e-9630-487f-998d-bb6ee0a31dc0"
   },
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze and display results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Calculate MSE and timing for each method\n",
    "    mse_results = {}\n",
    "    timing_results = {}\n",
    "\n",
    "    for method_name, method_results in results.items():\n",
    "        estimates_data = method_results['estimates']\n",
    "\n",
    "        # 计算训练成本分摊\n",
    "        training_time = method_results['training_time']\n",
    "        n_training_samples = method_results['n_training_samples']\n",
    "        if n_training_samples > 0:\n",
    "            training_cost_per_sample = training_time / n_training_samples\n",
    "        else:\n",
    "            training_cost_per_sample = 0\n",
    "\n",
    "        # Group by theta value\n",
    "        theta_groups = {}\n",
    "        theta_times = {}\n",
    "\n",
    "        for i, est in enumerate(estimates_data):\n",
    "            theta = est['true_theta']\n",
    "            if theta not in theta_groups:\n",
    "                theta_groups[theta] = []\n",
    "                theta_times[theta] = []\n",
    "            theta_groups[theta].append(est['estimate'])\n",
    "            theta_times[theta].append(method_results['inference_times'][i])\n",
    "\n",
    "        # Calculate MSE and costs for each theta\n",
    "        method_mse = {}\n",
    "        method_total_costs = {}\n",
    "        for theta in theta_groups.keys():\n",
    "            estimates = theta_groups[theta]\n",
    "            mse = np.mean([(est - theta)**2 for est in estimates])\n",
    "            method_mse[theta] = mse\n",
    "\n",
    "            inference_time = np.mean(theta_times[theta])\n",
    "            total_cost = training_cost_per_sample + inference_time\n",
    "            method_total_costs[theta] = total_cost\n",
    "\n",
    "        mse_results[method_name] = method_mse\n",
    "        timing_results[method_name] = method_total_costs\n",
    "\n",
    "        # Print results for each theta\n",
    "        print(f\"\\n{method_name.upper()}:\")\n",
    "        for theta in sorted(method_mse.keys()):\n",
    "            inference_time = np.mean(theta_times[theta])\n",
    "            print(f\"  θ={theta}: MSE={method_mse[theta]:.4f}\")\n",
    "            print(f\"           Training Cost={training_cost_per_sample:.6f}s, Inference Cost={inference_time:.6f}s, Total Cost={method_total_costs[theta]:.6f}s\")\n",
    "\n",
    "    # 计算相对效率\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RELATIVE EFFICIENCY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 找到基准方法 (通常选择一个作为参考)\n",
    "    baseline_method = list(mse_results.keys())[0]  # 使用第一个方法作为基准\n",
    "\n",
    "    for theta in sorted(mse_results[baseline_method].keys()):\n",
    "        print(f\"\\nθ={theta}:\")\n",
    "        baseline_mse = mse_results[baseline_method][theta]\n",
    "        baseline_cost = timing_results[baseline_method][theta]\n",
    "\n",
    "        for method_name in mse_results.keys():\n",
    "            method_mse = mse_results[method_name][theta]\n",
    "            method_cost = timing_results[method_name][theta]\n",
    "\n",
    "            # 相对效率 = (baseline_mse / method_mse) / (method_cost / baseline_cost)\n",
    "            # = (baseline_mse / method_mse) * (baseline_cost / method_cost)\n",
    "            relative_efficiency = (baseline_mse / method_mse) * (baseline_cost / method_cost)\n",
    "\n",
    "            print(f\"  {method_name}: Relative Efficiency = {relative_efficiency:.3f}\")\n",
    "\n",
    "    return mse_results, timing_results\n",
    "\n",
    "# 修改绘图函数\n",
    "def plot_comparison_results(mse_results, timing_results):\n",
    "    \"\"\"Plot comparison results\"\"\"\n",
    "    if not CONFIG['plot_results']:\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # MSE plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for method_name, method_mse in mse_results.items():\n",
    "        thetas = sorted(method_mse.keys())\n",
    "        mses = [method_mse[theta] for theta in thetas]\n",
    "        plt.plot(thetas, mses, 'o-', label=method_name, linewidth=2)\n",
    "\n",
    "    plt.xlabel('True θ')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Mean Squared Error vs True θ')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Cost plot - 显示平均成本\n",
    "    plt.subplot(1, 2, 2)\n",
    "    methods = list(timing_results.keys())\n",
    "    # 计算每个方法的平均成本\n",
    "    avg_costs = [np.mean(list(timing_results[method].values())) for method in methods]\n",
    "\n",
    "    plt.bar(methods, avg_costs)\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Average Total Cost per Sample (s)')\n",
    "    plt.title('Average Computational Cost Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# 调用\n",
    "mse_results, cost_results = analyze_results(experiment_results)\n",
    "plot_comparison_results(mse_results, cost_results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
